{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Name: Khor Yu Yang<br>Admin Number: 2123590"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Name: Nevan Ang Kai Wen<br>Admin Number: 2122867<br>Class: DAAA/FT/2B/01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGXZpBFMBYiX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gym, gym.wrappers\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPthCXu7BYid"
      },
      "source": [
        "<h1>Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pbcGL60BYig"
      },
      "source": [
        "<h3>Implementation of Dueling Deep Q Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63Rkpt3BYih"
      },
      "source": [
        "<h4>Baseline Model w/o Tuning/Improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddlWkazOBYij"
      },
      "outputs": [],
      "source": [
        "# Dueling DQN network\n",
        "#! handles estimation of action function\n",
        "class DuelingDeepQNetwork(tf.keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims):\n",
        "        super(DuelingDeepQNetwork, self).__init__()\n",
        "        self.dense1 = Dense(fc1_dims, activation='relu')\n",
        "        self.dense2 = Dense(fc2_dims, activation='relu')\n",
        "        self.V = Dense(1,activation=None) #value of state, returns raw value\n",
        "        # Advantage Layer\n",
        "        self.A = Dense(n_actions, activation=None)\n",
        "\n",
        "    # Feed forward method\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        # getting Q-value\n",
        "        Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
        "\n",
        "        return Q\n",
        "\n",
        "    # Advantage method for choosing actions\n",
        "    def advantage(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        return A\n",
        "\n",
        "# Replay Buffer\n",
        "#! Stores/keep track of the experience of agent, stores memory of the state,reward...etc\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        # Max size of memory\n",
        "        self.mem_size = max_size\n",
        "        self.mem_counter = 0\n",
        "        # State/New state/action/reward starting with 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "    \n",
        "    # Store transitions\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_ # new state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_counter += 1 # increment counter\n",
        "\n",
        "    # make sure agent learns from saved memory\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        # sample batch of memory\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        # returning sample experience of that batch\n",
        "        states = self.state_memory[batch]\n",
        "        new_states = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, new_states, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-l-y6m6BYil"
      },
      "source": [
        "Q-value is derived from the value and advantage, V & A layers return two separate outputs to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XALkbywHBYim"
      },
      "source": [
        "<h3>Agent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNrMGbzBBYin"
      },
      "source": [
        "The agent will consists of the memory (ReplayBuffer) and the dueling DQN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW3K9qZ1BYip"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "#! Learn from experiences/transitions, update the model\n",
        "class Agent():\n",
        "    def __init__(self,lr, gamma, n_actions, epsilon, batch_size, input_dims, eps_decay=1e-3, eps_min=0.01,\n",
        "    mem_size=100000, file_name='dueling_dqn.h5', fc1_dims=256,fc2_dims=256,replace=100):\n",
        "        # action = 4\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        # discount factor\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay # epsilon decay\n",
        "        self.eps_min = eps_min # min epsilon value to stop decay\n",
        "        self.file_name = file_name\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.learn_step_counter = 0\n",
        "        # Memory\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)\n",
        "        # Target Network with simialr architecture\n",
        "        self.q_next = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)\n",
        "\n",
        "        # Compile models\n",
        "        self.q_eval.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "        self.q_next.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "    # # interface function between memory and agent\n",
        "    # def store_transition(self, state, action, reward, new_state, done):\n",
        "    #     self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # # Function to choose action based on observations/environment\n",
        "    # def choose_action(self, observation):\n",
        "    #     # allow model to explore\n",
        "    #     if np.random.random() < self.epsilon:\n",
        "    #         action = np.random.choice(self.action_space)\n",
        "    #     else:\n",
        "    #         # greedy action\n",
        "    #         state = np.array([observation])\n",
        "    #         actions = self.q_eval.advantage(state)\n",
        "    #         action = tf.math.argmax(actions, axis=1).numpy()[0] #select best action\n",
        "        \n",
        "    #     return action\n",
        "    #  interface function between memory and agent\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # Function to choose action based on observations/environment\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # allow model to explore\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            # greedy action\n",
        "            state = np.array([observation])\n",
        "            actions = self.q_eval.advantage(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "    # Learning function\n",
        "    def learn(self):\n",
        "\n",
        "        #! allow model to start learning only after memory is filled up, not 0s\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Update target model weights\n",
        "        if self.learn_step_counter % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
        "        # print(states)\n",
        "        q_pred = self.q_eval(states)\n",
        "        # print(states_)\n",
        "        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_pred)\n",
        "\n",
        "        # improve on my solution!\n",
        "        for idx, terminal in enumerate(dones):\n",
        "            if terminal:\n",
        "                q_next[idx] = 0.0\n",
        "            # for action taken, is current reward + value of next step*gamma\n",
        "            q_target[idx, actions[idx]] = rewards[idx] + self.gamma*q_next[idx]\n",
        "\n",
        "        self.q_eval.train_on_batch(states, q_target)\n",
        "\n",
        "        #decay epsilon unless it reaches min\n",
        "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
        "        #when update parameters for target network\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "    # saving the model and loading the model\n",
        "    def save_model(self):\n",
        "        tf.keras.models.save_model(self.q_eval, 'baseline_network.h5')\n",
        "        # self.q_eval.save_weights('baseline_network.h5')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_eval = load_model(self.model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KLr47bPBYiq"
      },
      "source": [
        "<h3>Plot Learning Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC4KwN70BYir"
      },
      "outputs": [],
      "source": [
        "# function to plot learning curve\n",
        "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, label='1')\n",
        "    ax2 = fig.add_subplot(111, label='2', frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color='C0')\n",
        "    ax.set_xlabel('Game', color='C0')\n",
        "    ax.set_ylabel('Epsilon', color='C0')\n",
        "    ax.tick_params(axis='x', color='C0')\n",
        "    ax.tick_params(axis='y', color='C0')\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "    ax2.scatter(x, running_avg, color='C1')\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_ylabel('Score', color='C1')\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    ax2.tick_params(axis='y', color='C1')\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDzCzqy65mxk"
      },
      "outputs": [],
      "source": [
        "def moving_average(a, n=100) : #calculating the average of a certain number of data points over a fixed period of time.\n",
        "    #average of a certain number of consecutive data points, and then sliding the window of data points over the entire dataset, \n",
        "    #updating the average as the window moves. \n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "def plot_scores(scores):\n",
        "    scores_ma=moving_average(scores,n=100)\n",
        "    plt.plot(np.arange(len(scores_ma)), scores_ma)\n",
        "    plt.title('Tuned Model Learning Curve During Training')\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGUV_gTUBYis"
      },
      "source": [
        "<h3>Main Training Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jh4xnZmSBYit",
        "outputId": "7e3cea09-cc52-44bb-d1ae-d620d8b9fe2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-48-34f036171c81>:46: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/300, score: -174.73654058853174, average: -174.74, epsilon: 0.59\n",
            "episode: 1/300, score: -95.75978389787673, average: -135.25, epsilon: 0.10\n",
            "episode: 2/300, score: -52.48418266675166, average: -107.66, epsilon: 0.10\n",
            "episode: 3/300, score: -172.78486320219056, average: -123.94, epsilon: 0.10\n",
            "episode: 4/300, score: -40.89981039873855, average: -107.33, epsilon: 0.10\n",
            "episode: 5/300, score: -85.89976568639574, average: -103.76, epsilon: 0.10\n",
            "episode: 6/300, score: -563.8173277216665, average: -169.48, epsilon: 0.10\n",
            "episode: 7/300, score: -113.40347463796765, average: -162.47, epsilon: 0.10\n",
            "episode: 8/300, score: -134.43556632449324, average: -159.36, epsilon: 0.10\n",
            "episode: 9/300, score: -142.52492800592927, average: -157.67, epsilon: 0.10\n",
            "episode: 10/300, score: 43.212522073704406, average: -139.41, epsilon: 0.10\n",
            "episode: 11/300, score: -33.07441073946127, average: -130.55, epsilon: 0.10\n",
            "episode: 12/300, score: -68.9275075817647, average: -125.81, epsilon: 0.10\n",
            "episode: 13/300, score: -54.557423467039975, average: -120.72, epsilon: 0.10\n",
            "episode: 14/300, score: 58.761932539987555, average: -108.76, epsilon: 0.10\n",
            "episode: 15/300, score: -70.1441016425077, average: -106.34, epsilon: 0.10\n",
            "episode: 16/300, score: -70.63949681528885, average: -104.24, epsilon: 0.10\n",
            "episode: 17/300, score: -86.79579160374317, average: -103.27, epsilon: 0.10\n",
            "episode: 18/300, score: -102.50473080739947, average: -103.23, epsilon: 0.10\n",
            "episode: 19/300, score: -42.90944287486148, average: -100.22, epsilon: 0.10\n",
            "episode: 20/300, score: -104.7116871765967, average: -100.43, epsilon: 0.10\n",
            "episode: 21/300, score: -350.51523361759405, average: -111.80, epsilon: 0.10\n",
            "episode: 22/300, score: -81.61333436940716, average: -110.49, epsilon: 0.10\n",
            "episode: 23/300, score: -69.95050807233183, average: -108.80, epsilon: 0.10\n",
            "episode: 24/300, score: -51.180933060893935, average: -106.49, epsilon: 0.10\n",
            "episode: 25/300, score: -417.9151026248162, average: -118.47, epsilon: 0.10\n",
            "episode: 26/300, score: -149.1814625769501, average: -119.61, epsilon: 0.10\n",
            "episode: 27/300, score: -310.7076724711711, average: -126.43, epsilon: 0.10\n",
            "episode: 28/300, score: -191.14964910365984, average: -128.66, epsilon: 0.10\n",
            "episode: 29/300, score: -141.44988245472072, average: -129.09, epsilon: 0.10\n",
            "episode: 30/300, score: -86.7061093877474, average: -127.72, epsilon: 0.10\n",
            "episode: 31/300, score: -18.249677520196443, average: -124.30, epsilon: 0.10\n",
            "episode: 32/300, score: -14.745297794286586, average: -120.98, epsilon: 0.10\n",
            "episode: 33/300, score: -15.603967476750213, average: -117.88, epsilon: 0.10\n",
            "episode: 34/300, score: -525.9510853471883, average: -129.54, epsilon: 0.10\n",
            "episode: 35/300, score: -211.49824853716885, average: -131.82, epsilon: 0.10\n",
            "episode: 36/300, score: -52.28872222285171, average: -129.67, epsilon: 0.10\n",
            "episode: 37/300, score: -38.686238646872006, average: -127.27, epsilon: 0.10\n",
            "episode: 38/300, score: -215.08369243684547, average: -129.53, epsilon: 0.10\n",
            "episode: 39/300, score: -147.5896063995266, average: -129.98, epsilon: 0.10\n",
            "episode: 40/300, score: -288.11412078360297, average: -133.83, epsilon: 0.10\n",
            "episode: 41/300, score: -251.2496160655255, average: -136.63, epsilon: 0.10\n",
            "episode: 42/300, score: -345.356051555484, average: -141.48, epsilon: 0.10\n",
            "episode: 43/300, score: -333.9736555037277, average: -145.86, epsilon: 0.10\n",
            "episode: 44/300, score: -154.08880794276882, average: -146.04, epsilon: 0.10\n",
            "episode: 45/300, score: -269.33473871370177, average: -148.72, epsilon: 0.10\n",
            "episode: 46/300, score: -381.78246171776476, average: -153.68, epsilon: 0.10\n",
            "episode: 47/300, score: -347.0355775499773, average: -157.71, epsilon: 0.10\n",
            "episode: 48/300, score: -610.3919050368638, average: -166.95, epsilon: 0.10\n",
            "episode: 49/300, score: -186.46463509528027, average: -167.34, epsilon: 0.10\n",
            "episode: 50/300, score: -62.616786557620316, average: -165.28, epsilon: 0.10\n",
            "episode: 51/300, score: -383.8852140531729, average: -169.49, epsilon: 0.10\n",
            "episode: 52/300, score: -651.2939258486422, average: -178.58, epsilon: 0.10\n",
            "episode: 53/300, score: -505.72576302368464, average: -184.64, epsilon: 0.10\n",
            "episode: 54/300, score: -449.5699153780792, average: -189.45, epsilon: 0.10\n",
            "episode: 55/300, score: -388.1029658727331, average: -193.00, epsilon: 0.10\n",
            "episode: 56/300, score: -507.18245602176125, average: -198.51, epsilon: 0.10\n",
            "episode: 57/300, score: -3.8081135677383884, average: -195.16, epsilon: 0.10\n",
            "episode: 58/300, score: -206.0331396296105, average: -195.34, epsilon: 0.10\n",
            "episode: 59/300, score: -337.4061231614904, average: -197.71, epsilon: 0.10\n",
            "episode: 60/300, score: -351.62960328121716, average: -200.23, epsilon: 0.10\n",
            "episode: 61/300, score: -271.056591765851, average: -201.37, epsilon: 0.10\n",
            "episode: 62/300, score: -89.14085944962935, average: -199.59, epsilon: 0.10\n",
            "episode: 63/300, score: -100.90995869340234, average: -198.05, epsilon: 0.10\n",
            "episode: 64/300, score: -97.17852502841298, average: -196.50, epsilon: 0.10\n",
            "episode: 65/300, score: -138.6505771243352, average: -195.62, epsilon: 0.10\n",
            "episode: 66/300, score: -237.7718658578452, average: -196.25, epsilon: 0.10\n",
            "episode: 67/300, score: -195.5490425299377, average: -196.24, epsilon: 0.10\n",
            "episode: 68/300, score: -295.3852234939628, average: -197.68, epsilon: 0.10\n",
            "episode: 69/300, score: -350.7307066004985, average: -199.86, epsilon: 0.10\n",
            "episode: 70/300, score: -139.24890355327682, average: -199.01, epsilon: 0.10\n",
            "episode: 71/300, score: -292.14570980306524, average: -200.30, epsilon: 0.10\n",
            "episode: 72/300, score: -129.95572799606177, average: -199.34, epsilon: 0.10\n",
            "episode: 73/300, score: -152.48439356552396, average: -198.71, epsilon: 0.10\n",
            "episode: 74/300, score: -338.33941757250244, average: -200.57, epsilon: 0.10\n",
            "episode: 75/300, score: -20.705307426675276, average: -198.20, epsilon: 0.10\n",
            "episode: 76/300, score: -72.30910990198781, average: -196.57, epsilon: 0.10\n",
            "episode: 77/300, score: -159.7372749747942, average: -196.10, epsilon: 0.10\n",
            "episode: 78/300, score: -247.54534153427076, average: -196.75, epsilon: 0.10\n",
            "episode: 79/300, score: -293.5976327864902, average: -197.96, epsilon: 0.10\n",
            "episode: 80/300, score: -90.29517168241475, average: -196.63, epsilon: 0.10\n",
            "episode: 81/300, score: -58.272349316172956, average: -194.94, epsilon: 0.10\n",
            "episode: 82/300, score: -165.61428457559418, average: -194.59, epsilon: 0.10\n",
            "episode: 83/300, score: -110.87148408936423, average: -193.59, epsilon: 0.10\n",
            "episode: 84/300, score: -187.81925061467916, average: -193.52, epsilon: 0.10\n",
            "episode: 85/300, score: -167.64737988329472, average: -193.22, epsilon: 0.10\n",
            "episode: 86/300, score: -225.24254173563492, average: -193.59, epsilon: 0.10\n",
            "episode: 87/300, score: -259.227475857126, average: -194.34, epsilon: 0.10\n",
            "episode: 88/300, score: -231.27925682486094, average: -194.75, epsilon: 0.10\n",
            "episode: 89/300, score: -269.33699010370634, average: -195.58, epsilon: 0.10\n",
            "episode: 90/300, score: -308.9575404069142, average: -196.83, epsilon: 0.10\n",
            "episode: 91/300, score: -81.03425872291953, average: -195.57, epsilon: 0.10\n",
            "episode: 92/300, score: 59.60700746323772, average: -192.82, epsilon: 0.10\n",
            "episode: 93/300, score: 41.0522052226691, average: -190.34, epsilon: 0.10\n",
            "episode: 94/300, score: 71.36402512829424, average: -187.58, epsilon: 0.10\n",
            "episode: 95/300, score: 84.17172896376796, average: -184.75, epsilon: 0.10\n",
            "episode: 96/300, score: -288.4819095840971, average: -185.82, epsilon: 0.10\n",
            "episode: 97/300, score: 83.61321025061721, average: -183.07, epsilon: 0.10\n",
            "episode: 98/300, score: -460.29434184697965, average: -185.87, epsilon: 0.10\n",
            "episode: 99/300, score: 31.579365076845285, average: -183.70, epsilon: 0.10\n",
            "episode: 100/300, score: -789.1108542391289, average: -189.84, epsilon: 0.10\n",
            "episode: 101/300, score: -9.594559189408669, average: -188.98, epsilon: 0.10\n",
            "episode: 102/300, score: -233.64997419459112, average: -190.79, epsilon: 0.10\n",
            "episode: 103/300, score: -13.814973994768849, average: -189.20, epsilon: 0.10\n",
            "episode: 104/300, score: 57.15906386489573, average: -188.22, epsilon: 0.10\n",
            "episode: 105/300, score: -577.5567062488988, average: -193.14, epsilon: 0.10\n",
            "episode: 106/300, score: -338.8441576916488, average: -190.89, epsilon: 0.10\n",
            "episode: 107/300, score: 89.26412924149321, average: -188.86, epsilon: 0.10\n",
            "episode: 108/300, score: 82.67894465125838, average: -186.69, epsilon: 0.10\n",
            "episode: 109/300, score: -63.13613604317758, average: -185.89, epsilon: 0.10\n",
            "episode: 110/300, score: -169.32052923268984, average: -188.02, epsilon: 0.10\n",
            "episode: 111/300, score: -129.8994286216651, average: -188.99, epsilon: 0.10\n",
            "episode: 112/300, score: -371.1389120691711, average: -192.01, epsilon: 0.10\n",
            "episode: 113/300, score: -405.70816859269985, average: -195.52, epsilon: 0.10\n",
            "episode: 114/300, score: -489.50141507673686, average: -201.00, epsilon: 0.10\n",
            "episode: 115/300, score: -315.19902956588925, average: -203.45, epsilon: 0.10\n",
            "episode: 116/300, score: -448.7859741768846, average: -207.24, epsilon: 0.10\n",
            "episode: 117/300, score: -169.67523044156175, average: -208.07, epsilon: 0.10\n",
            "episode: 118/300, score: -498.646183427038, average: -212.03, epsilon: 0.10\n",
            "episode: 119/300, score: -110.30012000916244, average: -212.70, epsilon: 0.10\n",
            "episode: 120/300, score: -306.11179870624755, average: -214.71, epsilon: 0.10\n",
            "episode: 121/300, score: -129.64567217174655, average: -212.51, epsilon: 0.10\n",
            "episode: 122/300, score: -459.28718897657876, average: -216.28, epsilon: 0.10\n",
            "episode: 123/300, score: -500.82233596123854, average: -220.59, epsilon: 0.10\n",
            "episode: 124/300, score: -424.57744635413064, average: -224.33, epsilon: 0.10\n",
            "episode: 125/300, score: -395.3892265201826, average: -224.10, epsilon: 0.10\n",
            "episode: 126/300, score: -151.38772921108492, average: -224.12, epsilon: 0.10\n",
            "episode: 127/300, score: -166.36594410119503, average: -222.68, epsilon: 0.10\n",
            "episode: 128/300, score: -123.88599369321503, average: -222.01, epsilon: 0.10\n",
            "episode: 129/300, score: -419.3296723421685, average: -224.78, epsilon: 0.10\n",
            "episode: 130/300, score: -213.46287457919627, average: -226.05, epsilon: 0.10\n",
            "episode: 131/300, score: -133.11495052040686, average: -227.20, epsilon: 0.10\n",
            "episode: 132/300, score: -158.80923307714286, average: -228.64, epsilon: 0.10\n",
            "episode: 133/300, score: -248.3144287031786, average: -230.97, epsilon: 0.10\n",
            "episode: 134/300, score: -332.7498731552662, average: -229.04, epsilon: 0.10\n",
            "episode: 135/300, score: -277.49051371180485, average: -229.70, epsilon: 0.10\n",
            "episode: 136/300, score: -897.0829500088164, average: -238.14, epsilon: 0.10\n",
            "episode: 137/300, score: -264.5787838604824, average: -240.40, epsilon: 0.10\n",
            "episode: 138/300, score: -595.7919552488798, average: -244.21, epsilon: 0.10\n",
            "episode: 139/300, score: -408.4257549786886, average: -246.82, epsilon: 0.10\n",
            "episode: 140/300, score: -177.4442251489758, average: -245.71, epsilon: 0.10\n",
            "episode: 141/300, score: -235.53722577754945, average: -245.55, epsilon: 0.10\n",
            "episode: 142/300, score: -1394.6082327486306, average: -256.05, epsilon: 0.10\n",
            "episode: 143/300, score: -539.5523452372953, average: -258.10, epsilon: 0.10\n",
            "episode: 144/300, score: -214.46629220015592, average: -258.71, epsilon: 0.10\n",
            "episode: 145/300, score: -545.7691528062974, average: -261.47, epsilon: 0.10\n",
            "episode: 146/300, score: -280.79425293826705, average: -260.46, epsilon: 0.10\n",
            "episode: 147/300, score: -132.82850874068492, average: -258.32, epsilon: 0.10\n",
            "episode: 148/300, score: -124.208290331529, average: -253.46, epsilon: 0.10\n",
            "episode: 149/300, score: -383.56309695842253, average: -255.43, epsilon: 0.10\n",
            "episode: 150/300, score: -218.90005967744412, average: -256.99, epsilon: 0.10\n",
            "episode: 151/300, score: -266.08414551778554, average: -255.81, epsilon: 0.10\n",
            "episode: 152/300, score: -209.5598167754531, average: -251.40, epsilon: 0.10\n",
            "episode: 153/300, score: -223.17954975553317, average: -248.57, epsilon: 0.10\n",
            "episode: 154/300, score: -271.1890583945938, average: -246.79, epsilon: 0.10\n",
            "episode: 155/300, score: -131.97611285287644, average: -244.23, epsilon: 0.10\n",
            "episode: 156/300, score: -176.58375005782918, average: -240.92, epsilon: 0.10\n",
            "episode: 157/300, score: -367.3629822367347, average: -244.56, epsilon: 0.10\n",
            "episode: 158/300, score: -200.27158057062803, average: -244.50, epsilon: 0.10\n",
            "episode: 159/300, score: -176.38170259509684, average: -242.89, epsilon: 0.10\n",
            "episode: 160/300, score: -170.010543028522, average: -241.07, epsilon: 0.10\n",
            "episode: 161/300, score: -260.6240905866297, average: -240.97, epsilon: 0.10\n",
            "episode: 162/300, score: -577.6421858169399, average: -245.85, epsilon: 0.10\n",
            "episode: 163/300, score: -285.39791188910715, average: -247.70, epsilon: 0.10\n",
            "episode: 164/300, score: -53.60982873020593, average: -247.26, epsilon: 0.10\n",
            "episode: 165/300, score: -246.9714642263381, average: -248.34, epsilon: 0.10\n",
            "episode: 166/300, score: -264.24801664028234, average: -248.61, epsilon: 0.10\n",
            "episode: 167/300, score: -177.8838872052878, average: -248.43, epsilon: 0.10\n",
            "episode: 168/300, score: -377.53175015743847, average: -249.25, epsilon: 0.10\n",
            "episode: 169/300, score: -235.03186562414032, average: -248.10, epsilon: 0.10\n",
            "episode: 170/300, score: -227.5007955755637, average: -248.98, epsilon: 0.10\n",
            "episode: 171/300, score: -229.24902880014804, average: -248.35, epsilon: 0.10\n",
            "episode: 172/300, score: -95.49010289864356, average: -248.01, epsilon: 0.10\n",
            "episode: 173/300, score: -115.90666924313899, average: -247.64, epsilon: 0.10\n",
            "episode: 174/300, score: -262.3203482290402, average: -246.88, epsilon: 0.10\n",
            "episode: 175/300, score: -192.96982284636033, average: -248.60, epsilon: 0.10\n",
            "episode: 176/300, score: -155.22374518092593, average: -249.43, epsilon: 0.10\n",
            "episode: 177/300, score: -153.24558557314856, average: -249.37, epsilon: 0.10\n",
            "episode: 178/300, score: -139.40397608799384, average: -248.29, epsilon: 0.10\n",
            "episode: 179/300, score: -192.01347227396815, average: -247.27, epsilon: 0.10\n",
            "episode: 180/300, score: -153.90305356074958, average: -247.91, epsilon: 0.10\n",
            "episode: 181/300, score: -193.78200945033467, average: -249.26, epsilon: 0.10\n",
            "episode: 182/300, score: -155.36178940535618, average: -249.16, epsilon: 0.10\n",
            "episode: 183/300, score: -152.98843060175437, average: -249.58, epsilon: 0.10\n",
            "episode: 184/300, score: -172.71520139543827, average: -249.43, epsilon: 0.10\n",
            "episode: 185/300, score: -142.01879104890304, average: -249.17, epsilon: 0.10\n",
            "episode: 186/300, score: -286.32338385711, average: -249.78, epsilon: 0.10\n",
            "episode: 187/300, score: -69.61550723018895, average: -247.89, epsilon: 0.10\n",
            "episode: 188/300, score: -282.4223714575227, average: -248.40, epsilon: 0.10\n",
            "episode: 189/300, score: -177.1710934737133, average: -247.48, epsilon: 0.10\n",
            "episode: 190/300, score: -259.84596520641753, average: -246.99, epsilon: 0.10\n",
            "episode: 191/300, score: -240.97593062797736, average: -248.58, epsilon: 0.10\n",
            "episode: 192/300, score: -186.08650308565802, average: -251.04, epsilon: 0.10\n",
            "episode: 193/300, score: -223.79618451701685, average: -253.69, epsilon: 0.10\n",
            "episode: 194/300, score: -101.05364096559028, average: -255.41, epsilon: 0.10\n",
            "episode: 195/300, score: -318.522599912062, average: -259.44, epsilon: 0.10\n",
            "episode: 196/300, score: -313.5905278560269, average: -259.69, epsilon: 0.10\n",
            "episode: 197/300, score: -160.2935302359598, average: -262.13, epsilon: 0.10\n",
            "episode: 198/300, score: -348.4657852006278, average: -261.01, epsilon: 0.10\n",
            "episode: 199/300, score: -579.1497036442681, average: -267.12, epsilon: 0.10\n",
            "episode: 200/300, score: -365.2014043038374, average: -262.88, epsilon: 0.10\n",
            "episode: 201/300, score: -287.1639730718574, average: -265.66, epsilon: 0.10\n",
            "episode: 202/300, score: -365.2447956526374, average: -266.97, epsilon: 0.10\n",
            "episode: 203/300, score: -169.58524094447407, average: -268.53, epsilon: 0.10\n",
            "episode: 204/300, score: -134.02171976195282, average: -270.44, epsilon: 0.10\n",
            "episode: 205/300, score: -245.90768453055802, average: -267.13, epsilon: 0.10\n",
            "episode: 206/300, score: -51.94528272324338, average: -264.26, epsilon: 0.10\n",
            "episode: 207/300, score: -212.04970880705184, average: -267.27, epsilon: 0.10\n",
            "episode: 208/300, score: -134.13809032386706, average: -269.44, epsilon: 0.10\n",
            "episode: 209/300, score: -237.23957239733284, average: -271.18, epsilon: 0.10\n",
            "episode: 210/300, score: -315.83209708694824, average: -272.64, epsilon: 0.10\n",
            "episode: 211/300, score: -160.28357197319087, average: -272.95, epsilon: 0.10\n",
            "episode: 212/300, score: -366.59431123531675, average: -272.90, epsilon: 0.10\n",
            "episode: 213/300, score: -301.0915677099669, average: -271.86, epsilon: 0.10\n",
            "episode: 214/300, score: -143.2677796566487, average: -268.39, epsilon: 0.10\n",
            "episode: 215/300, score: -328.8905694823359, average: -268.53, epsilon: 0.10\n",
            "episode: 216/300, score: -227.46027963340003, average: -266.32, epsilon: 0.10\n",
            "episode: 217/300, score: -355.95303412185535, average: -268.18, epsilon: 0.10\n",
            "episode: 218/300, score: -186.2712733254872, average: -265.06, epsilon: 0.10\n",
            "episode: 219/300, score: -454.2472645945221, average: -268.50, epsilon: 0.10\n",
            "episode: 220/300, score: -195.17346715665656, average: -267.39, epsilon: 0.10\n",
            "episode: 221/300, score: -537.9100576138762, average: -271.47, epsilon: 0.10\n",
            "episode: 222/300, score: -348.18071764331364, average: -270.36, epsilon: 0.10\n",
            "episode: 223/300, score: -127.82956015571672, average: -266.63, epsilon: 0.10\n",
            "episode: 224/300, score: -233.91221269939857, average: -264.72, epsilon: 0.10\n",
            "episode: 225/300, score: -87.89039512200704, average: -261.65, epsilon: 0.10\n",
            "episode: 226/300, score: -236.82873660226116, average: -262.50, epsilon: 0.10\n",
            "episode: 227/300, score: -139.87288778120922, average: -262.24, epsilon: 0.10\n",
            "episode: 228/300, score: -201.74132500672624, average: -263.01, epsilon: 0.10\n",
            "episode: 229/300, score: -191.65247115420732, average: -260.74, epsilon: 0.10\n",
            "episode: 230/300, score: -223.6833746627383, average: -260.84, epsilon: 0.10\n",
            "episode: 231/300, score: -200.5062771127479, average: -261.51, epsilon: 0.10\n",
            "episode: 232/300, score: -191.67486910042726, average: -261.84, epsilon: 0.10\n",
            "episode: 233/300, score: -89.33573328282978, average: -260.25, epsilon: 0.10\n",
            "episode: 234/300, score: -131.82271109281385, average: -258.24, epsilon: 0.10\n",
            "episode: 235/300, score: -235.52282284479506, average: -257.82, epsilon: 0.10\n",
            "episode: 236/300, score: -188.0324381652727, average: -250.73, epsilon: 0.10\n",
            "episode: 237/300, score: -124.94707144875142, average: -249.34, epsilon: 0.10\n",
            "episode: 238/300, score: -33.55148081890201, average: -243.72, epsilon: 0.10\n",
            "episode: 239/300, score: 215.14993477165874, average: -237.48, epsilon: 0.10\n",
            "episode: 240/300, score: -328.9894445319135, average: -238.99, epsilon: 0.10\n",
            "episode: 241/300, score: -239.63086154957398, average: -239.04, epsilon: 0.10\n",
            "episode: 242/300, score: -424.26835827511593, average: -229.33, epsilon: 0.10\n",
            "episode: 243/300, score: -662.2407737243436, average: -230.56, epsilon: 0.10\n",
            "episode: 244/300, score: -878.4536515806086, average: -237.20, epsilon: 0.10\n",
            "episode: 245/300, score: -386.55295090812876, average: -235.61, epsilon: 0.10\n",
            "episode: 246/300, score: -1179.0299316886199, average: -244.59, epsilon: 0.10\n",
            "episode: 247/300, score: -729.6652609948884, average: -250.56, epsilon: 0.10\n",
            "episode: 248/300, score: -1113.1488646843459, average: -260.45, epsilon: 0.10\n",
            "episode: 249/300, score: -951.226379187444, average: -266.12, epsilon: 0.10\n",
            "episode: 250/300, score: -857.1958248410509, average: -272.51, epsilon: 0.10\n",
            "episode: 251/300, score: -1728.6506155522081, average: -287.13, epsilon: 0.10\n",
            "episode: 252/300, score: -1161.2929416942893, average: -296.65, epsilon: 0.10\n",
            "episode: 253/300, score: -630.770082355995, average: -300.73, epsilon: 0.10\n",
            "episode: 254/300, score: -278.3398029180417, average: -300.80, epsilon: 0.10\n",
            "episode: 255/300, score: -73.0744835817111, average: -300.21, epsilon: 0.10\n",
            "episode: 256/300, score: -1149.5161464992639, average: -309.94, epsilon: 0.10\n",
            "episode: 257/300, score: -63.09489536182942, average: -306.89, epsilon: 0.10\n",
            "episode: 258/300, score: -658.7991191065332, average: -311.48, epsilon: 0.10\n",
            "episode: 259/300, score: -281.0793812815448, average: -312.53, epsilon: 0.10\n",
            "episode: 260/300, score: -33.238621669862965, average: -311.16, epsilon: 0.10\n",
            "episode: 261/300, score: -52.58312480081074, average: -309.08, epsilon: 0.10\n",
            "episode: 262/300, score: -73.47767770771597, average: -304.04, epsilon: 0.10\n",
            "episode: 263/300, score: -68.00662071598438, average: -301.86, epsilon: 0.10\n",
            "episode: 264/300, score: -40.474195531170935, average: -301.73, epsilon: 0.10\n",
            "episode: 265/300, score: -621.5429572196251, average: -305.48, epsilon: 0.10\n",
            "episode: 266/300, score: -351.83868012574135, average: -306.35, epsilon: 0.10\n",
            "episode: 267/300, score: -125.7842685192084, average: -305.83, epsilon: 0.10\n",
            "episode: 268/300, score: -378.02400166621834, average: -305.84, epsilon: 0.10\n",
            "episode: 269/300, score: -137.5082001294617, average: -304.86, epsilon: 0.10\n",
            "episode: 270/300, score: -96.42132940444473, average: -303.55, epsilon: 0.10\n",
            "episode: 271/300, score: -149.36221544737606, average: -302.75, epsilon: 0.10\n",
            "episode: 272/300, score: -162.5799988800138, average: -303.42, epsilon: 0.10\n",
            "episode: 273/300, score: -219.68602226586103, average: -304.46, epsilon: 0.10\n",
            "episode: 274/300, score: -127.65787738524466, average: -303.11, epsilon: 0.10\n",
            "episode: 275/300, score: -245.67257223291907, average: -303.64, epsilon: 0.10\n",
            "episode: 276/300, score: -135.604180129887, average: -303.45, epsilon: 0.10\n",
            "episode: 277/300, score: -43.27232207081943, average: -302.35, epsilon: 0.10\n",
            "episode: 278/300, score: 185.41103059633417, average: -299.10, epsilon: 0.10\n",
            "episode: 279/300, score: 144.8564104628311, average: -295.73, epsilon: 0.10\n",
            "episode: 280/300, score: 2.6278870668845924, average: -294.16, epsilon: 0.10\n",
            "episode: 281/300, score: -232.62331486309805, average: -294.55, epsilon: 0.10\n",
            "episode: 282/300, score: 89.56903346122003, average: -292.10, epsilon: 0.10\n",
            "episode: 283/300, score: -110.2558696854116, average: -291.68, epsilon: 0.10\n",
            "episode: 284/300, score: -84.92131920583992, average: -290.80, epsilon: 0.10\n",
            "episode: 285/300, score: -80.39698594909731, average: -290.18, epsilon: 0.10\n",
            "episode: 286/300, score: 23.75849251851958, average: -287.08, epsilon: 0.10\n",
            "episode: 287/300, score: -277.5671661202522, average: -289.16, epsilon: 0.10\n",
            "episode: 288/300, score: 3.4735531300940607, average: -286.30, epsilon: 0.10\n",
            "episode: 289/300, score: -82.35494357437953, average: -285.35, epsilon: 0.10\n",
            "episode: 290/300, score: -11.964502015988998, average: -282.87, epsilon: 0.10\n",
            "episode: 291/300, score: -263.99012410735094, average: -283.10, epsilon: 0.10\n",
            "episode: 292/300, score: -86.65340498349782, average: -282.11, epsilon: 0.10\n",
            "episode: 293/300, score: 172.9422974016229, average: -278.14, epsilon: 0.10\n",
            "episode: 294/300, score: -351.05157079911027, average: -280.64, epsilon: 0.10\n",
            "episode: 295/300, score: -29.003365823006167, average: -277.75, epsilon: 0.10\n",
            "episode: 296/300, score: -81.91578990291688, average: -275.43, epsilon: 0.10\n",
            "episode: 297/300, score: -157.84877025663502, average: -275.41, epsilon: 0.10\n",
            "episode: 298/300, score: 110.76688133419421, average: -270.81, epsilon: 0.10\n",
            "episode: 299/300, score: -183.1210369001304, average: -266.85, epsilon: 0.10\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "n_episodes = 300\n",
        "agent = Agent(gamma=0.99, epsilon=1, lr=0.01, input_dims=[8], eps_decay=0.01,mem_size=100000,\n",
        "            batch_size=64,eps_min=0.1,fc1_dims=128,fc2_dims=128,replace=100,n_actions=4)\n",
        "\n",
        "# define score and epsilon history\n",
        "scores, eps_history = [], []\n",
        "\n",
        "for i in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        agent.store_transition(observation, action, reward, observation_, done)\n",
        "        observation = observation_\n",
        "        agent.learn()\n",
        "\n",
        "    eps_history.append(agent.epsilon)\n",
        "    scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "    print(\"episode: {}/{}, score: {}, average: {:.2f}, epsilon: {:.2f}\".format(\n",
        "        i, n_episodes, score, avg_score, agent.epsilon))\n",
        "\n",
        "file_name = 'BaseLine_LunarLander.png'\n",
        "agent.q_eval.save_weights('baseline_network.h5')\n",
        "\n",
        "# Show learning curves\n",
        "x = [i+1 for i in range(n_episodes)]\n",
        "plot_learning_curve(x, scores, eps_history, file_name) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5V7jBzQBYiu"
      },
      "source": [
        "the model performance is not good, average reward at -369.75 after 300 episodes. It is not learning well. Since it is only the baseline model, we would want to modified it to make it perform better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pewY4u0MBYiv"
      },
      "source": [
        "<h3>Modify Model Network with additional hidden layer, more filters per layer to test its performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhkcBLuEBYiw"
      },
      "outputs": [],
      "source": [
        "# Increase number of filters for Deuling DQN network\n",
        "# Dueling DQN network\n",
        "#! handles estimation of action function\n",
        "class DuelingDeepQNetwork(tf.keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims, fc3_dims):\n",
        "        super(DuelingDeepQNetwork, self).__init__()\n",
        "        self.dense1 = Dense(fc1_dims, activation='relu')\n",
        "        self.dense2 = Dense(fc2_dims, activation='relu')\n",
        "        self.dense3 = Dense(fc3_dims, activation='relu')\n",
        "        self.V = Dense(1,activation=None) #value of state, returns raw value\n",
        "        # Advantage Layer\n",
        "        self.A = Dense(n_actions, activation=None)\n",
        "\n",
        "    # Feed forward method\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        # getting Q-value\n",
        "        Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
        "\n",
        "        return Q\n",
        "\n",
        "    # Advantage method for choosing actions\n",
        "    def advantage(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        return A\n",
        "\n",
        "# Replay Buffer\n",
        "#! Stores/keep track of the experience of agent, stores memory of the state,reward...etc\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        # Max size of memory\n",
        "        self.mem_size = max_size\n",
        "        self.mem_counter = 0\n",
        "        # State/New state/action/reward starting with 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "    \n",
        "    # Store transitions\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_ # new state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_counter += 1 # increment counter\n",
        "\n",
        "    # make sure agent learns from saved memory\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        # sample batch of memory\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        # returning sample experience of that batch\n",
        "        states = self.state_memory[batch]\n",
        "        new_states = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, new_states, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHR-yDW3BYix"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "#! Learn from experiences/transitions, update the model\n",
        "class Agent():\n",
        "    def __init__(self,lr, gamma, n_actions, epsilon, batch_size, input_dims, eps_decay=1e-3, eps_min=0.01,\n",
        "    mem_size=100000, file_name='dueling_dqn.h5', fc1_dims=256,fc2_dims=256,fc3_dims=256,replace=100):\n",
        "        # action = 4\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        # discount factor\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay # epsilon decay\n",
        "        self.eps_min = eps_min # min epsilon value to stop decay\n",
        "        self.file_name = file_name\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.learn_step_counter = 0\n",
        "        # Memory\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims,fc3_dims)\n",
        "        # Target Network with simialr architecture\n",
        "        self.q_next = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims, fc3_dims)\n",
        "\n",
        "        # Compile models\n",
        "        self.q_eval.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "        self.q_next.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "    # # interface function between memory and agent\n",
        "    # def store_transition(self, state, action, reward, new_state, done):\n",
        "    #     self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # # Function to choose action based on observations/environment\n",
        "    # def choose_action(self, observation):\n",
        "    #     # allow model to explore\n",
        "    #     if np.random.random() < self.epsilon:\n",
        "    #         action = np.random.choice(self.action_space)\n",
        "    #     else:\n",
        "    #         # greedy action\n",
        "    #         state = np.array([observation])\n",
        "    #         actions = self.q_eval.advantage(state)\n",
        "    #         action = tf.math.argmax(actions, axis=1).numpy()[0] #select best action\n",
        "        \n",
        "    #     return action\n",
        "    #  interface function between memory and agent\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # Function to choose action based on observations/environment\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # allow model to explore\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            # greedy action\n",
        "            state = np.array([observation])\n",
        "            actions = self.q_eval.advantage(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "    # Learning function\n",
        "    def learn(self):\n",
        "\n",
        "        #! allow model to start learning only after memory is filled up, not 0s\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Update target model weights\n",
        "        if self.learn_step_counter % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
        "        # print(states)\n",
        "        q_pred = self.q_eval(states)\n",
        "        # print(states_)\n",
        "        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_pred)\n",
        "\n",
        "        # improve on my solution!\n",
        "        for idx, terminal in enumerate(dones):\n",
        "            if terminal:\n",
        "                q_next[idx] = 0.0\n",
        "            # for action taken, is current reward + value of next step*gamma\n",
        "            q_target[idx, actions[idx]] = rewards[idx] + self.gamma*q_next[idx]\n",
        "\n",
        "        self.q_eval.train_on_batch(states, q_target)\n",
        "\n",
        "        #decay epsilon unless it reaches min\n",
        "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
        "        #when update parameters for target network\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "    # saving the model and loading the model\n",
        "    def save_model(self, title='modified_network.h5'):\n",
        "        self.q_eval.save(title)\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_eval = load_model(self.model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX4v2pr7BYix"
      },
      "source": [
        "<h3>Training Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F68WGELcBYiy",
        "outputId": "531779d5-65b8-412a-d016-f4d6d361be25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-426806dcdffc>:50: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/300, score: -153.8589525330272, average: -153.86, epsilon: 0.60\n",
            "episode: 1/300, score: -212.36712710773378, average: -183.11, epsilon: 0.10\n",
            "episode: 2/300, score: -195.76698282368045, average: -187.33, epsilon: 0.10\n",
            "episode: 3/300, score: 236.7523234971918, average: -81.31, epsilon: 0.10\n",
            "episode: 4/300, score: -64.3224318157409, average: -77.91, epsilon: 0.10\n",
            "episode: 5/300, score: -346.1946942781202, average: -122.63, epsilon: 0.10\n",
            "episode: 6/300, score: -46.206638493472994, average: -111.71, epsilon: 0.10\n",
            "episode: 7/300, score: -167.85440545976542, average: -118.73, epsilon: 0.10\n",
            "episode: 8/300, score: 21.171515897066456, average: -103.18, epsilon: 0.10\n",
            "episode: 9/300, score: -135.3246942687135, average: -106.40, epsilon: 0.10\n",
            "episode: 10/300, score: -85.40079552412637, average: -104.49, epsilon: 0.10\n",
            "episode: 11/300, score: 140.30485099239326, average: -84.09, epsilon: 0.10\n",
            "episode: 12/300, score: -212.16974580827176, average: -93.94, epsilon: 0.10\n",
            "episode: 13/300, score: -333.70056548264404, average: -111.07, epsilon: 0.10\n",
            "episode: 14/300, score: -132.92456605009428, average: -112.52, epsilon: 0.10\n",
            "episode: 15/300, score: -273.1375174048686, average: -122.56, epsilon: 0.10\n",
            "episode: 16/300, score: -32.09540811525386, average: -117.24, epsilon: 0.10\n",
            "episode: 17/300, score: 54.87807696461164, average: -107.68, epsilon: 0.10\n",
            "episode: 18/300, score: -15.957451427261745, average: -102.85, epsilon: 0.10\n",
            "episode: 19/300, score: -678.1913277975324, average: -131.62, epsilon: 0.10\n",
            "episode: 20/300, score: -105.17964164027258, average: -130.36, epsilon: 0.10\n",
            "episode: 21/300, score: -128.88713028458784, average: -130.29, epsilon: 0.10\n",
            "episode: 22/300, score: -79.54355282213443, average: -128.09, epsilon: 0.10\n",
            "episode: 23/300, score: -31.364572722814145, average: -124.06, epsilon: 0.10\n",
            "episode: 24/300, score: -338.23302803507556, average: -132.62, epsilon: 0.10\n",
            "episode: 25/300, score: -194.18898090286066, average: -134.99, epsilon: 0.10\n",
            "episode: 26/300, score: -100.88783623765661, average: -133.73, epsilon: 0.10\n",
            "episode: 27/300, score: -91.42099029687971, average: -132.22, epsilon: 0.10\n",
            "episode: 28/300, score: -44.609652573438694, average: -129.20, epsilon: 0.10\n",
            "episode: 29/300, score: -156.29649030994733, average: -130.10, epsilon: 0.10\n",
            "episode: 30/300, score: -294.1905655749911, average: -135.39, epsilon: 0.10\n",
            "episode: 31/300, score: -76.329090347365, average: -133.55, epsilon: 0.10\n",
            "episode: 32/300, score: -21.873747937452848, average: -130.16, epsilon: 0.10\n",
            "episode: 33/300, score: -82.07356433355476, average: -128.75, epsilon: 0.10\n",
            "episode: 34/300, score: -192.88112459634482, average: -130.58, epsilon: 0.10\n",
            "episode: 35/300, score: -6.815782890556349, average: -127.14, epsilon: 0.10\n",
            "episode: 36/300, score: 14.076877071129076, average: -123.33, epsilon: 0.10\n",
            "episode: 37/300, score: -25.559239057222342, average: -120.75, epsilon: 0.10\n",
            "episode: 38/300, score: 0.752726428620889, average: -117.64, epsilon: 0.10\n",
            "episode: 39/300, score: -95.17159067847219, average: -117.08, epsilon: 0.10\n",
            "episode: 40/300, score: 7.048632891413902, average: -114.05, epsilon: 0.10\n",
            "episode: 41/300, score: -118.99285056632161, average: -114.17, epsilon: 0.10\n",
            "episode: 42/300, score: 7.68209302874223, average: -111.33, epsilon: 0.10\n",
            "episode: 43/300, score: -184.86240970611425, average: -113.00, epsilon: 0.10\n",
            "episode: 44/300, score: -151.95331618978355, average: -113.87, epsilon: 0.10\n",
            "episode: 45/300, score: 21.297905099010507, average: -110.93, epsilon: 0.10\n",
            "episode: 46/300, score: -158.43007917295859, average: -111.94, epsilon: 0.10\n",
            "episode: 47/300, score: -10.676866826072981, average: -109.83, epsilon: 0.10\n",
            "episode: 48/300, score: -30.81700722446608, average: -108.22, epsilon: 0.10\n",
            "episode: 49/300, score: -97.98627945361714, average: -108.01, epsilon: 0.10\n",
            "episode: 50/300, score: 115.14722551667288, average: -103.64, epsilon: 0.10\n",
            "episode: 51/300, score: -80.60523099649201, average: -103.20, epsilon: 0.10\n",
            "episode: 52/300, score: -140.89537655234875, average: -103.91, epsilon: 0.10\n",
            "episode: 53/300, score: -225.81072455574076, average: -106.16, epsilon: 0.10\n",
            "episode: 54/300, score: -84.5179529177778, average: -105.77, epsilon: 0.10\n",
            "episode: 55/300, score: -169.49824137129923, average: -106.91, epsilon: 0.10\n",
            "episode: 56/300, score: -36.97214516403191, average: -105.68, epsilon: 0.10\n",
            "episode: 57/300, score: -59.253130195165795, average: -104.88, epsilon: 0.10\n",
            "episode: 58/300, score: -164.03788385711547, average: -105.88, epsilon: 0.10\n",
            "episode: 59/300, score: -64.43889309573001, average: -105.19, epsilon: 0.10\n",
            "episode: 60/300, score: -408.35009681778143, average: -110.16, epsilon: 0.10\n",
            "episode: 61/300, score: -21.91393924714349, average: -108.74, epsilon: 0.10\n",
            "episode: 62/300, score: -321.8516690673656, average: -112.12, epsilon: 0.10\n",
            "episode: 63/300, score: -793.0906739977086, average: -122.76, epsilon: 0.10\n",
            "episode: 64/300, score: -5.1971008045890725, average: -120.95, epsilon: 0.10\n",
            "episode: 65/300, score: -82.2755985780314, average: -120.37, epsilon: 0.10\n",
            "episode: 66/300, score: -285.6889304797621, average: -122.84, epsilon: 0.10\n",
            "episode: 67/300, score: 83.05838347607789, average: -119.81, epsilon: 0.10\n",
            "episode: 68/300, score: -35.489322258598065, average: -118.59, epsilon: 0.10\n",
            "episode: 69/300, score: -18.340174430830743, average: -117.15, epsilon: 0.10\n",
            "episode: 70/300, score: -105.96647499428995, average: -117.00, epsilon: 0.10\n",
            "episode: 71/300, score: -220.19741880309445, average: -118.43, epsilon: 0.10\n",
            "episode: 72/300, score: -132.61363253732694, average: -118.62, epsilon: 0.10\n",
            "episode: 73/300, score: -106.76585528848015, average: -118.46, epsilon: 0.10\n",
            "episode: 74/300, score: -278.07838042412334, average: -120.59, epsilon: 0.10\n",
            "episode: 75/300, score: -27.79266210148012, average: -119.37, epsilon: 0.10\n",
            "episode: 76/300, score: -200.34970503137896, average: -120.42, epsilon: 0.10\n",
            "episode: 77/300, score: 140.6701902513712, average: -117.07, epsilon: 0.10\n",
            "episode: 78/300, score: -242.21350846244206, average: -118.66, epsilon: 0.10\n",
            "episode: 79/300, score: -183.7777430114893, average: -119.47, epsilon: 0.10\n",
            "episode: 80/300, score: -144.65041832969453, average: -119.78, epsilon: 0.10\n",
            "episode: 81/300, score: -477.67825800969, average: -124.15, epsilon: 0.10\n",
            "episode: 82/300, score: -432.5198683741357, average: -127.86, epsilon: 0.10\n",
            "episode: 83/300, score: -7.751466719848679, average: -126.43, epsilon: 0.10\n",
            "episode: 84/300, score: -555.0273790431812, average: -131.48, epsilon: 0.10\n",
            "episode: 85/300, score: 288.9962972767362, average: -126.59, epsilon: 0.10\n",
            "episode: 86/300, score: -29.866767143258016, average: -125.48, epsilon: 0.10\n",
            "episode: 87/300, score: -40.71450445170824, average: -124.51, epsilon: 0.10\n",
            "episode: 88/300, score: 6.693280096118812, average: -123.04, epsilon: 0.10\n",
            "episode: 89/300, score: -407.8820903230818, average: -126.20, epsilon: 0.10\n",
            "episode: 90/300, score: -527.9492867873286, average: -130.62, epsilon: 0.10\n",
            "episode: 91/300, score: -90.2109577832048, average: -130.18, epsilon: 0.10\n",
            "episode: 92/300, score: -485.2495555194358, average: -134.00, epsilon: 0.10\n",
            "episode: 93/300, score: -547.3684895096685, average: -138.39, epsilon: 0.10\n",
            "episode: 94/300, score: -171.75861189951138, average: -138.75, epsilon: 0.10\n",
            "episode: 95/300, score: -263.1931689164216, average: -140.04, epsilon: 0.10\n",
            "episode: 96/300, score: -503.4382744381504, average: -143.79, epsilon: 0.10\n",
            "episode: 97/300, score: -430.086667126946, average: -146.71, epsilon: 0.10\n",
            "episode: 98/300, score: -348.9600972146927, average: -148.75, epsilon: 0.10\n",
            "episode: 99/300, score: -393.9488726242004, average: -151.20, epsilon: 0.10\n",
            "episode: 100/300, score: -699.2144948886563, average: -156.66, epsilon: 0.10\n",
            "episode: 101/300, score: -575.856854158813, average: -160.29, epsilon: 0.10\n",
            "episode: 102/300, score: -420.81125032886047, average: -162.54, epsilon: 0.10\n",
            "episode: 103/300, score: -429.4570817470765, average: -169.21, epsilon: 0.10\n",
            "episode: 104/300, score: -424.64448664221777, average: -172.81, epsilon: 0.10\n",
            "episode: 105/300, score: -417.83527596504825, average: -173.52, epsilon: 0.10\n",
            "episode: 106/300, score: -478.4328604845632, average: -177.85, epsilon: 0.10\n",
            "episode: 107/300, score: -388.6718384696568, average: -180.06, epsilon: 0.10\n",
            "episode: 108/300, score: -262.24941969698256, average: -182.89, epsilon: 0.10\n",
            "episode: 109/300, score: -374.14506824754153, average: -185.28, epsilon: 0.10\n",
            "episode: 110/300, score: -434.2646421332913, average: -188.77, epsilon: 0.10\n",
            "episode: 111/300, score: -217.333365262619, average: -192.34, epsilon: 0.10\n",
            "episode: 112/300, score: -640.5698230585905, average: -196.63, epsilon: 0.10\n",
            "episode: 113/300, score: -507.4017975181624, average: -198.36, epsilon: 0.10\n",
            "episode: 114/300, score: -703.9089753354868, average: -204.07, epsilon: 0.10\n",
            "episode: 115/300, score: -427.792748432062, average: -205.62, epsilon: 0.10\n",
            "episode: 116/300, score: -222.16366565813465, average: -207.52, epsilon: 0.10\n",
            "episode: 117/300, score: -388.4523247703991, average: -211.95, epsilon: 0.10\n",
            "episode: 118/300, score: -440.3480678195552, average: -216.20, epsilon: 0.10\n",
            "episode: 119/300, score: -538.1895519035118, average: -214.80, epsilon: 0.10\n",
            "episode: 120/300, score: -596.532813656665, average: -219.71, epsilon: 0.10\n",
            "episode: 121/300, score: -482.2767529934665, average: -223.25, epsilon: 0.10\n",
            "episode: 122/300, score: -491.0674333868157, average: -227.36, epsilon: 0.10\n",
            "episode: 123/300, score: -367.9324911700043, average: -230.73, epsilon: 0.10\n",
            "episode: 124/300, score: -644.6038665399549, average: -233.79, epsilon: 0.10\n",
            "episode: 125/300, score: -657.7414777188925, average: -238.43, epsilon: 0.10\n",
            "episode: 126/300, score: -508.2335140976576, average: -242.50, epsilon: 0.10\n",
            "episode: 127/300, score: -445.7463275139369, average: -246.04, epsilon: 0.10\n",
            "episode: 128/300, score: -327.7600076755753, average: -248.87, epsilon: 0.10\n",
            "episode: 129/300, score: -415.4695066920078, average: -251.47, epsilon: 0.10\n",
            "episode: 130/300, score: -441.9842887713336, average: -252.94, epsilon: 0.10\n",
            "episode: 131/300, score: -378.1342945176906, average: -255.96, epsilon: 0.10\n",
            "episode: 132/300, score: -493.0876891057559, average: -260.67, epsilon: 0.10\n",
            "episode: 133/300, score: -302.95184410931563, average: -262.88, epsilon: 0.10\n",
            "episode: 134/300, score: -403.8514049885254, average: -264.99, epsilon: 0.10\n",
            "episode: 135/300, score: -330.8811660483966, average: -268.23, epsilon: 0.10\n",
            "episode: 136/300, score: -342.41286550692445, average: -271.80, epsilon: 0.10\n",
            "episode: 137/300, score: -384.3708097357004, average: -275.39, epsilon: 0.10\n",
            "episode: 138/300, score: -365.7299870860971, average: -279.05, epsilon: 0.10\n",
            "episode: 139/300, score: -460.35706717978206, average: -282.70, epsilon: 0.10\n",
            "episode: 140/300, score: -382.5644415283711, average: -286.60, epsilon: 0.10\n",
            "episode: 141/300, score: -248.6996476291031, average: -287.90, epsilon: 0.10\n",
            "episode: 142/300, score: -461.9825871395611, average: -292.59, epsilon: 0.10\n",
            "episode: 143/300, score: -593.1538232939993, average: -296.68, epsilon: 0.10\n",
            "episode: 144/300, score: -317.99384791658434, average: -298.34, epsilon: 0.10\n",
            "episode: 145/300, score: -415.17376476701793, average: -302.70, epsilon: 0.10\n",
            "episode: 146/300, score: -253.33488480015913, average: -303.65, epsilon: 0.10\n",
            "episode: 147/300, score: -575.0275354213138, average: -309.29, epsilon: 0.10\n",
            "episode: 148/300, score: -347.93506110404496, average: -312.46, epsilon: 0.10\n",
            "episode: 149/300, score: -299.3425872391367, average: -314.48, epsilon: 0.10\n",
            "episode: 150/300, score: -630.9153876173025, average: -321.94, epsilon: 0.10\n",
            "episode: 151/300, score: -427.9702148449369, average: -325.41, epsilon: 0.10\n",
            "episode: 152/300, score: -211.18304951044604, average: -326.11, epsilon: 0.10\n",
            "episode: 153/300, score: -423.3637138526196, average: -328.09, epsilon: 0.10\n",
            "episode: 154/300, score: -378.4369916360529, average: -331.03, epsilon: 0.10\n",
            "episode: 155/300, score: -285.4372186385881, average: -332.19, epsilon: 0.10\n",
            "episode: 156/300, score: -295.70103380523426, average: -334.78, epsilon: 0.10\n",
            "episode: 157/300, score: -461.75099953171457, average: -338.80, epsilon: 0.10\n",
            "episode: 158/300, score: -334.2694538278265, average: -340.50, epsilon: 0.10\n",
            "episode: 159/300, score: -377.9026753439714, average: -343.64, epsilon: 0.10\n",
            "episode: 160/300, score: -425.61855414844746, average: -343.81, epsilon: 0.10\n",
            "episode: 161/300, score: -499.43364663128955, average: -348.59, epsilon: 0.10\n",
            "episode: 162/300, score: -441.9347170667738, average: -349.79, epsilon: 0.10\n",
            "episode: 163/300, score: -657.338625666133, average: -348.43, epsilon: 0.10\n",
            "episode: 164/300, score: -610.3926974808012, average: -354.48, epsilon: 0.10\n",
            "episode: 165/300, score: -393.31001550443267, average: -357.59, epsilon: 0.10\n",
            "episode: 166/300, score: -556.5769979924205, average: -360.30, epsilon: 0.10\n",
            "episode: 167/300, score: -621.9822194021507, average: -367.35, epsilon: 0.10\n",
            "episode: 168/300, score: -231.62595572258854, average: -369.31, epsilon: 0.10\n",
            "episode: 169/300, score: -425.4190750205733, average: -373.38, epsilon: 0.10\n",
            "episode: 170/300, score: -475.0259663337204, average: -377.07, epsilon: 0.10\n",
            "episode: 171/300, score: -349.0556723694919, average: -378.36, epsilon: 0.10\n",
            "episode: 172/300, score: -426.99677432849603, average: -381.31, epsilon: 0.10\n",
            "episode: 173/300, score: -542.5764941349057, average: -385.66, epsilon: 0.10\n",
            "episode: 174/300, score: -544.6342970386868, average: -388.33, epsilon: 0.10\n",
            "episode: 175/300, score: -414.7346425944983, average: -392.20, epsilon: 0.10\n",
            "episode: 176/300, score: -541.8977599351044, average: -395.61, epsilon: 0.10\n",
            "episode: 177/300, score: -580.5237944778479, average: -402.83, epsilon: 0.10\n",
            "episode: 178/300, score: -632.0988627973795, average: -406.73, epsilon: 0.10\n",
            "episode: 179/300, score: -1153.1187332999841, average: -416.42, epsilon: 0.10\n",
            "episode: 180/300, score: -507.1850272282307, average: -420.04, epsilon: 0.10\n",
            "episode: 181/300, score: -518.32998650933, average: -420.45, epsilon: 0.10\n",
            "episode: 182/300, score: -519.1625133745945, average: -421.32, epsilon: 0.10\n",
            "episode: 183/300, score: -954.2043416272933, average: -430.78, epsilon: 0.10\n",
            "episode: 184/300, score: -998.6603248272679, average: -435.22, epsilon: 0.10\n",
            "episode: 185/300, score: -323.179494879247, average: -441.34, epsilon: 0.10\n",
            "episode: 186/300, score: -1256.7324329752862, average: -453.61, epsilon: 0.10\n",
            "episode: 187/300, score: -758.3119857160234, average: -460.78, epsilon: 0.10\n",
            "episode: 188/300, score: -545.9688221091263, average: -466.31, epsilon: 0.10\n",
            "episode: 189/300, score: -1017.5622619945161, average: -472.41, epsilon: 0.10\n",
            "episode: 190/300, score: -675.3146321672609, average: -473.88, epsilon: 0.10\n",
            "episode: 191/300, score: -592.472466705686, average: -478.90, epsilon: 0.10\n",
            "episode: 192/300, score: -691.9074524360356, average: -480.97, epsilon: 0.10\n",
            "episode: 193/300, score: -682.0862508305047, average: -482.32, epsilon: 0.10\n",
            "episode: 194/300, score: -781.8275764769553, average: -488.42, epsilon: 0.10\n",
            "episode: 195/300, score: -1353.4503532035403, average: -499.32, epsilon: 0.10\n",
            "episode: 196/300, score: -897.6522726219383, average: -503.26, epsilon: 0.10\n",
            "episode: 197/300, score: -396.462373508804, average: -502.93, epsilon: 0.10\n",
            "episode: 198/300, score: -1304.6615627654915, average: -512.48, epsilon: 0.10\n",
            "episode: 199/300, score: -933.6657566668453, average: -517.88, epsilon: 0.10\n",
            "episode: 200/300, score: -1183.1090398340025, average: -522.72, epsilon: 0.10\n",
            "episode: 201/300, score: -1227.3295388222903, average: -529.23, epsilon: 0.10\n",
            "episode: 202/300, score: -974.4936634689981, average: -534.77, epsilon: 0.10\n",
            "episode: 203/300, score: -355.90125658240333, average: -534.04, epsilon: 0.10\n",
            "episode: 204/300, score: -1019.4997553497992, average: -539.98, epsilon: 0.10\n",
            "episode: 205/300, score: -1381.576992814182, average: -549.62, epsilon: 0.10\n",
            "episode: 206/300, score: -889.5402196549092, average: -553.73, epsilon: 0.10\n",
            "episode: 207/300, score: -245.57190047120363, average: -552.30, epsilon: 0.10\n",
            "episode: 208/300, score: -769.4409947891374, average: -557.37, epsilon: 0.10\n",
            "episode: 209/300, score: -2404.5140519977986, average: -577.68, epsilon: 0.10\n",
            "episode: 210/300, score: -629.5868134101727, average: -579.63, epsilon: 0.10\n",
            "episode: 211/300, score: -435.2773454352692, average: -581.81, epsilon: 0.10\n",
            "episode: 212/300, score: -1498.8269504750244, average: -590.39, epsilon: 0.10\n",
            "episode: 213/300, score: -805.9749605339777, average: -593.38, epsilon: 0.10\n",
            "episode: 214/300, score: -726.1072119341467, average: -593.60, epsilon: 0.10\n",
            "episode: 215/300, score: -225.1630146997027, average: -591.57, epsilon: 0.10\n",
            "episode: 216/300, score: -410.4567680442294, average: -593.46, epsilon: 0.10\n",
            "episode: 217/300, score: -538.7975488270118, average: -594.96, epsilon: 0.10\n",
            "episode: 218/300, score: -470.99981522946405, average: -595.27, epsilon: 0.10\n",
            "episode: 219/300, score: -407.401893974841, average: -593.96, epsilon: 0.10\n",
            "episode: 220/300, score: -538.8948767455506, average: -593.38, epsilon: 0.10\n",
            "episode: 221/300, score: -1030.5588649139017, average: -598.87, epsilon: 0.10\n",
            "episode: 222/300, score: -620.0378890614819, average: -600.16, epsilon: 0.10\n",
            "episode: 223/300, score: -516.2092174580573, average: -601.64, epsilon: 0.10\n",
            "episode: 224/300, score: -558.0956082109658, average: -600.77, epsilon: 0.10\n",
            "episode: 225/300, score: -1268.0574359050831, average: -606.88, epsilon: 0.10\n",
            "episode: 226/300, score: -257.1176218887318, average: -604.37, epsilon: 0.10\n",
            "episode: 227/300, score: -839.7879745688138, average: -608.31, epsilon: 0.10\n",
            "episode: 228/300, score: -1635.2488440822633, average: -621.38, epsilon: 0.10\n",
            "episode: 229/300, score: -1643.766797281266, average: -633.66, epsilon: 0.10\n",
            "episode: 230/300, score: -679.912583031969, average: -636.04, epsilon: 0.10\n",
            "episode: 231/300, score: -263.80736313762156, average: -634.90, epsilon: 0.10\n",
            "episode: 232/300, score: -268.10136821811443, average: -632.65, epsilon: 0.10\n",
            "episode: 233/300, score: -491.325998723745, average: -634.53, epsilon: 0.10\n",
            "episode: 234/300, score: -526.1732659916144, average: -635.76, epsilon: 0.10\n",
            "episode: 235/300, score: -377.951732067893, average: -636.23, epsilon: 0.10\n",
            "episode: 236/300, score: -346.80185152987366, average: -636.27, epsilon: 0.10\n",
            "episode: 237/300, score: -1038.366576798061, average: -642.81, epsilon: 0.10\n",
            "episode: 238/300, score: -233.63414112632614, average: -641.49, epsilon: 0.10\n",
            "episode: 239/300, score: -317.8020997757559, average: -640.06, epsilon: 0.10\n",
            "episode: 240/300, score: -477.8462394367878, average: -641.02, epsilon: 0.10\n",
            "episode: 241/300, score: -311.1099776880218, average: -641.64, epsilon: 0.10\n",
            "episode: 242/300, score: -323.1669211853346, average: -640.25, epsilon: 0.10\n",
            "episode: 243/300, score: -418.7642814484014, average: -638.51, epsilon: 0.10\n",
            "episode: 244/300, score: -174.7909928929284, average: -637.08, epsilon: 0.10\n",
            "episode: 245/300, score: -1440.039397236799, average: -647.33, epsilon: 0.10\n",
            "episode: 246/300, score: -270.66819501444905, average: -647.50, epsilon: 0.10\n",
            "episode: 247/300, score: -375.8951601362123, average: -645.51, epsilon: 0.10\n",
            "episode: 248/300, score: -345.04186412114893, average: -645.48, epsilon: 0.10\n",
            "episode: 249/300, score: -197.1911604240658, average: -644.46, epsilon: 0.10\n",
            "episode: 250/300, score: -232.85650464700313, average: -640.48, epsilon: 0.10\n",
            "episode: 251/300, score: -68.53391916551028, average: -636.88, epsilon: 0.10\n",
            "episode: 252/300, score: -442.8727351569859, average: -639.20, epsilon: 0.10\n",
            "episode: 253/300, score: -547.8140872209867, average: -640.44, epsilon: 0.10\n",
            "episode: 254/300, score: -691.8494942426196, average: -643.58, epsilon: 0.10\n",
            "episode: 255/300, score: -206.8254039490683, average: -642.79, epsilon: 0.10\n",
            "episode: 256/300, score: -340.8934895044781, average: -643.24, epsilon: 0.10\n",
            "episode: 257/300, score: -416.03723459169305, average: -642.79, epsilon: 0.10\n",
            "episode: 258/300, score: -378.2292308737005, average: -643.23, epsilon: 0.10\n",
            "episode: 259/300, score: -451.85142119166125, average: -643.97, epsilon: 0.10\n",
            "episode: 260/300, score: -341.2196619772222, average: -643.12, epsilon: 0.10\n",
            "episode: 261/300, score: -333.0507162107317, average: -641.46, epsilon: 0.10\n",
            "episode: 262/300, score: -401.6867346600824, average: -641.06, epsilon: 0.10\n",
            "episode: 263/300, score: -429.1776830942534, average: -638.77, epsilon: 0.10\n",
            "episode: 264/300, score: -247.1454822936901, average: -635.14, epsilon: 0.10\n",
            "episode: 265/300, score: -295.40009080967303, average: -634.16, epsilon: 0.10\n",
            "episode: 266/300, score: -1794.1707493549889, average: -646.54, epsilon: 0.10\n",
            "episode: 267/300, score: -433.59874247030064, average: -644.65, epsilon: 0.10\n",
            "episode: 268/300, score: -391.5413617856712, average: -646.25, epsilon: 0.10\n",
            "episode: 269/300, score: -444.4299486235053, average: -646.44, epsilon: 0.10\n",
            "episode: 270/300, score: -1882.7270248357997, average: -660.52, epsilon: 0.10\n",
            "episode: 271/300, score: -219.686517855629, average: -659.23, epsilon: 0.10\n",
            "episode: 272/300, score: -168.97030585125208, average: -656.65, epsilon: 0.10\n",
            "episode: 273/300, score: -203.49595293984032, average: -653.26, epsilon: 0.10\n",
            "episode: 274/300, score: -446.0260848731111, average: -652.27, epsilon: 0.10\n",
            "episode: 275/300, score: -136.8103893140365, average: -649.49, epsilon: 0.10\n",
            "episode: 276/300, score: -269.5230614603643, average: -646.77, epsilon: 0.10\n",
            "episode: 277/300, score: -297.75246465231953, average: -643.94, epsilon: 0.10\n",
            "episode: 278/300, score: -234.98148442436027, average: -639.97, epsilon: 0.10\n",
            "episode: 279/300, score: -365.3178685907813, average: -632.09, epsilon: 0.10\n",
            "episode: 280/300, score: -256.79827433335345, average: -629.59, epsilon: 0.10\n",
            "episode: 281/300, score: -242.73863799160364, average: -626.83, epsilon: 0.10\n",
            "episode: 282/300, score: -2358.306846951705, average: -645.22, epsilon: 0.10\n",
            "episode: 283/300, score: -198.36577638288173, average: -637.66, epsilon: 0.10\n",
            "episode: 284/300, score: -397.756375468172, average: -631.65, epsilon: 0.10\n",
            "episode: 285/300, score: -250.6011340965753, average: -630.93, epsilon: 0.10\n",
            "episode: 286/300, score: -211.8613882363362, average: -620.48, epsilon: 0.10\n",
            "episode: 287/300, score: -335.3185293240733, average: -616.25, epsilon: 0.10\n",
            "episode: 288/300, score: -387.6116024531152, average: -614.67, epsilon: 0.10\n",
            "episode: 289/300, score: -286.9909774554065, average: -607.36, epsilon: 0.10\n",
            "episode: 290/300, score: -418.24762606100836, average: -604.79, epsilon: 0.10\n",
            "episode: 291/300, score: -112.27244097362359, average: -599.99, epsilon: 0.10\n",
            "episode: 292/300, score: -387.10796725740653, average: -596.94, epsilon: 0.10\n",
            "episode: 293/300, score: -463.3922306375805, average: -594.75, epsilon: 0.10\n",
            "episode: 294/300, score: -408.39469715567174, average: -591.02, epsilon: 0.10\n",
            "episode: 295/300, score: -273.93339816756713, average: -580.22, epsilon: 0.10\n",
            "episode: 296/300, score: -368.75925284521037, average: -574.93, epsilon: 0.10\n",
            "episode: 297/300, score: -478.59635601713154, average: -575.76, epsilon: 0.10\n",
            "episode: 298/300, score: -352.27149632007024, average: -566.23, epsilon: 0.10\n",
            "episode: 299/300, score: -295.10586502977185, average: -559.85, epsilon: 0.10\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "n_episodes = 300\n",
        "agent = Agent(gamma=0.99, epsilon=1, lr=0.01, input_dims=[8], eps_decay=0.01,mem_size=100000,\n",
        "            batch_size=64,eps_min=0.1,fc1_dims=256,fc2_dims=256,fc3_dims=256,replace=100,n_actions=4)\n",
        "\n",
        "# define score and epsilon history\n",
        "scores, eps_history = [], []\n",
        "\n",
        "for i in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        agent.store_transition(observation, action, reward, observation_, done)\n",
        "        observation = observation_\n",
        "        agent.learn()\n",
        "\n",
        "    eps_history.append(agent.epsilon)\n",
        "    scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "    print(\"episode: {}/{}, score: {}, average: {:.2f}, epsilon: {:.2f}\".format(\n",
        "        i, n_episodes, score, avg_score, agent.epsilon))\n",
        "\n",
        "file_name = 'modfiied_LunarLander.png'\n",
        "agent.q_eval.save_weights('modified_network.h5')\n",
        "\n",
        "# Show learning curves\n",
        "x = [i+1 for i in range(n_episodes)]\n",
        "plot_learning_curve(x, scores, eps_history, file_name) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpXaegT-BYiz"
      },
      "source": [
        "The result shown that the average reward got worser with increasing layers and filter. Therefore, we reduced it back to the original layers and tune its hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NO-oxQQBYiz"
      },
      "source": [
        "<h3>Model After Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrYbOKJ9YXGK"
      },
      "outputs": [],
      "source": [
        "# Dueling DQN network\n",
        "#! handles estimation of action function\n",
        "class DuelingDeepQNetwork(tf.keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims, fc2_dims):\n",
        "        super(DuelingDeepQNetwork, self).__init__()\n",
        "        self.dense1 = Dense(fc1_dims, activation='relu')\n",
        "        self.dense2 = Dense(fc2_dims, activation='relu')\n",
        "        self.V = Dense(1,activation=None) #value of state, returns raw value\n",
        "        # Advantage Layer\n",
        "        self.A = Dense(n_actions, activation=None)\n",
        "\n",
        "    # Feed forward method\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        # getting Q-value\n",
        "        Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
        "\n",
        "        return Q\n",
        "\n",
        "    # Advantage method for choosing actions\n",
        "    def advantage(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        A = self.A(x)\n",
        "\n",
        "        return A\n",
        "\n",
        "# Replay Buffer\n",
        "#! Stores/keep track of the experience of agent, stores memory of the state,reward...etc\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        # Max size of memory\n",
        "        self.mem_size = max_size\n",
        "        self.mem_counter = 0\n",
        "        # State/New state/action/reward starting with 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "    \n",
        "    # Store transitions\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_ # new state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_counter += 1 # increment counter\n",
        "\n",
        "    # make sure agent learns from saved memory\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        # sample batch of memory\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        # returning sample experience of that batch\n",
        "        states = self.state_memory[batch]\n",
        "        new_states = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, new_states, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHWDCCGNYdij"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "#! Learn from experiences/transitions, update the model\n",
        "class Agent():\n",
        "    def __init__(self,lr, gamma, n_actions, epsilon, batch_size, input_dims, eps_decay=1e-3, eps_min=0.01,\n",
        "    mem_size=100000, file_name='dueling_dqn.h5', fc1_dims=256,fc2_dims=256,replace=100):\n",
        "        # action = 4\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        # discount factor\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay # epsilon decay\n",
        "        self.eps_min = eps_min # min epsilon value to stop decay\n",
        "        self.file_name = file_name\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.learn_step_counter = 0\n",
        "        # Memory\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)\n",
        "        # Target Network with simialr architecture\n",
        "        self.q_next = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)\n",
        "\n",
        "        # Compile models\n",
        "        self.q_eval.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "        self.q_next.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "    # # interface function between memory and agent\n",
        "    # def store_transition(self, state, action, reward, new_state, done):\n",
        "    #     self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # # Function to choose action based on observations/environment\n",
        "    # def choose_action(self, observation):\n",
        "    #     # allow model to explore\n",
        "    #     if np.random.random() < self.epsilon:\n",
        "    #         action = np.random.choice(self.action_space)\n",
        "    #     else:\n",
        "    #         # greedy action\n",
        "    #         state = np.array([observation])\n",
        "    #         actions = self.q_eval.advantage(state)\n",
        "    #         action = tf.math.argmax(actions, axis=1).numpy()[0] #select best action\n",
        "        \n",
        "    #     return action\n",
        "    #  interface function between memory and agent\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # Function to choose action based on observations/environment\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # allow model to explore\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            # greedy action\n",
        "            state = np.array([observation])\n",
        "            actions = self.q_eval.advantage(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "    # Learning function\n",
        "    def learn(self):\n",
        "\n",
        "        #! allow model to start learning only after memory is filled up, not 0s\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Update target model weights\n",
        "        if self.learn_step_counter % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
        "        # print(states)\n",
        "        q_pred = self.q_eval(states)\n",
        "        # print(states_)\n",
        "        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_pred)\n",
        "\n",
        "        # improve on my solution!\n",
        "        for idx, terminal in enumerate(dones):\n",
        "            if terminal:\n",
        "                q_next[idx] = 0.0\n",
        "            # for action taken, is current reward + value of next step*gamma\n",
        "            q_target[idx, actions[idx]] = rewards[idx] + self.gamma*q_next[idx]\n",
        "\n",
        "        self.q_eval.train_on_batch(states, q_target)\n",
        "\n",
        "        #decay epsilon unless it reaches min\n",
        "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
        "        #when update parameters for target network\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "    # saving the model and loading the model\n",
        "    def save_model(self):\n",
        "        tf.keras.models.save_model(self.q_eval, 'baseline_network.h5')\n",
        "        # self.q_eval.save_weights('baseline_network.h5')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_eval = load_model(self.model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d8xABztlBYi0",
        "outputId": "5c0d3cfb-84ad-4e33-a657-33202f24dacb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-34f036171c81>:46: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/300, score: -119.34619507340157, average: -119.35, epsilon: 0.99\n",
            "episode: 1/300, score: -202.83045970231504, average: -161.09, epsilon: 0.91\n",
            "episode: 2/300, score: -83.01359579346148, average: -135.06, epsilon: 0.84\n",
            "episode: 3/300, score: -240.2683956066092, average: -161.36, epsilon: 0.69\n",
            "episode: 4/300, score: -188.10924172751072, average: -166.71, epsilon: 0.56\n",
            "episode: 5/300, score: -45.47616371298651, average: -146.51, epsilon: 0.40\n",
            "episode: 6/300, score: -302.0308910944513, average: -168.72, epsilon: 0.18\n",
            "episode: 7/300, score: -4.186305822113638, average: -148.16, epsilon: 0.01\n",
            "episode: 8/300, score: -118.35904133600434, average: -144.85, epsilon: 0.01\n",
            "episode: 9/300, score: -159.26095957891658, average: -146.29, epsilon: 0.01\n",
            "episode: 10/300, score: 193.65771612848636, average: -115.38, epsilon: 0.01\n",
            "episode: 11/300, score: -71.920491554189, average: -111.76, epsilon: 0.01\n",
            "episode: 12/300, score: -59.412269461093224, average: -107.74, epsilon: 0.01\n",
            "episode: 13/300, score: -93.07407564427776, average: -106.69, epsilon: 0.01\n",
            "episode: 14/300, score: -84.8628093978528, average: -105.23, epsilon: 0.01\n",
            "episode: 15/300, score: -84.78970857510627, average: -103.96, epsilon: 0.01\n",
            "episode: 16/300, score: -298.7091025218944, average: -115.41, epsilon: 0.01\n",
            "episode: 17/300, score: -253.0132477104404, average: -123.06, epsilon: 0.01\n",
            "episode: 18/300, score: -29.005998338719312, average: -118.11, epsilon: 0.01\n",
            "episode: 19/300, score: 8.244374894182972, average: -111.79, epsilon: 0.01\n",
            "episode: 20/300, score: -3.380323249571039, average: -106.63, epsilon: 0.01\n",
            "episode: 21/300, score: -23.700672062986587, average: -102.86, epsilon: 0.01\n",
            "episode: 22/300, score: -48.1403547715963, average: -100.48, epsilon: 0.01\n",
            "episode: 23/300, score: 94.15673408055962, average: -92.37, epsilon: 0.01\n",
            "episode: 24/300, score: -285.320658100865, average: -100.09, epsilon: 0.01\n",
            "episode: 25/300, score: -313.1808385556228, average: -108.28, epsilon: 0.01\n",
            "episode: 26/300, score: -160.76826085426683, average: -110.23, epsilon: 0.01\n",
            "episode: 27/300, score: -221.76530749184732, average: -114.21, epsilon: 0.01\n",
            "episode: 28/300, score: -254.47575116154354, average: -119.05, epsilon: 0.01\n",
            "episode: 29/300, score: 153.92535521502654, average: -109.95, epsilon: 0.01\n",
            "episode: 30/300, score: 132.71127609637648, average: -102.12, epsilon: 0.01\n",
            "episode: 31/300, score: -204.78601200946582, average: -105.33, epsilon: 0.01\n",
            "episode: 32/300, score: -238.65258902548015, average: -109.37, epsilon: 0.01\n",
            "episode: 33/300, score: -237.37158653858086, average: -113.13, epsilon: 0.01\n",
            "episode: 34/300, score: -100.16959056384316, average: -112.76, epsilon: 0.01\n",
            "episode: 35/300, score: 191.14919729918012, average: -104.32, epsilon: 0.01\n",
            "episode: 36/300, score: -248.8755173517876, average: -108.23, epsilon: 0.01\n",
            "episode: 37/300, score: -203.14859119644592, average: -110.73, epsilon: 0.01\n",
            "episode: 38/300, score: -40.54089967375307, average: -108.93, epsilon: 0.01\n",
            "episode: 39/300, score: -32.58505065014844, average: -107.02, epsilon: 0.01\n",
            "episode: 40/300, score: 100.01442039085487, average: -101.97, epsilon: 0.01\n",
            "episode: 41/300, score: -180.24895525629108, average: -103.83, epsilon: 0.01\n",
            "episode: 42/300, score: -95.6318764409296, average: -103.64, epsilon: 0.01\n",
            "episode: 43/300, score: -91.27513401162724, average: -103.36, epsilon: 0.01\n",
            "episode: 44/300, score: 158.49601116084267, average: -97.54, epsilon: 0.01\n",
            "episode: 45/300, score: 33.99936284120072, average: -94.68, epsilon: 0.01\n",
            "episode: 46/300, score: 131.46361981921348, average: -89.87, epsilon: 0.01\n",
            "episode: 47/300, score: 78.33990696283567, average: -86.37, epsilon: 0.01\n",
            "episode: 48/300, score: -16.085318045025602, average: -84.93, epsilon: 0.01\n",
            "episode: 49/300, score: 263.0391469828769, average: -77.97, epsilon: 0.01\n",
            "episode: 50/300, score: -158.31730999142394, average: -79.55, epsilon: 0.01\n",
            "episode: 51/300, score: -111.48395100695326, average: -80.16, epsilon: 0.01\n",
            "episode: 52/300, score: -173.96503065910156, average: -81.93, epsilon: 0.01\n",
            "episode: 53/300, score: -54.18892082323963, average: -81.42, epsilon: 0.01\n",
            "episode: 54/300, score: 25.42990639084566, average: -79.47, epsilon: 0.01\n",
            "episode: 55/300, score: -72.40904134296983, average: -79.35, epsilon: 0.01\n",
            "episode: 56/300, score: -69.73517173578661, average: -79.18, epsilon: 0.01\n",
            "episode: 57/300, score: -201.65034117883346, average: -81.29, epsilon: 0.01\n",
            "episode: 58/300, score: 123.24361456068114, average: -77.82, epsilon: 0.01\n",
            "episode: 59/300, score: -197.1503141707936, average: -79.81, epsilon: 0.01\n",
            "episode: 60/300, score: 80.4349517109903, average: -77.19, epsilon: 0.01\n",
            "episode: 61/300, score: -144.7090774483397, average: -78.28, epsilon: 0.01\n",
            "episode: 62/300, score: -11.83228501819157, average: -77.22, epsilon: 0.01\n",
            "episode: 63/300, score: -132.0003223326454, average: -78.08, epsilon: 0.01\n",
            "episode: 64/300, score: -49.95801413251018, average: -77.64, epsilon: 0.01\n",
            "episode: 65/300, score: -81.79186186680856, average: -77.71, epsilon: 0.01\n",
            "episode: 66/300, score: -92.84151675718634, average: -77.93, epsilon: 0.01\n",
            "episode: 67/300, score: -58.74336577142869, average: -77.65, epsilon: 0.01\n",
            "episode: 68/300, score: 131.4533004911932, average: -74.62, epsilon: 0.01\n",
            "episode: 69/300, score: -124.25894767760641, average: -75.33, epsilon: 0.01\n",
            "episode: 70/300, score: 79.29384629076102, average: -73.15, epsilon: 0.01\n",
            "episode: 71/300, score: 221.0768920630293, average: -69.06, epsilon: 0.01\n",
            "episode: 72/300, score: -92.32922029030009, average: -69.38, epsilon: 0.01\n",
            "episode: 73/300, score: 181.84015626815437, average: -65.99, epsilon: 0.01\n",
            "episode: 74/300, score: -158.1346401478279, average: -67.22, epsilon: 0.01\n",
            "episode: 75/300, score: -9.559204018127403, average: -66.46, epsilon: 0.01\n",
            "episode: 76/300, score: 204.03968112286435, average: -62.95, epsilon: 0.01\n",
            "episode: 77/300, score: 27.885212445679535, average: -61.78, epsilon: 0.01\n",
            "episode: 78/300, score: 122.41430703575372, average: -59.45, epsilon: 0.01\n",
            "episode: 79/300, score: 163.23531730659687, average: -56.67, epsilon: 0.01\n",
            "episode: 80/300, score: 39.79289869682132, average: -55.48, epsilon: 0.01\n",
            "episode: 81/300, score: 206.15240951545394, average: -52.28, epsilon: 0.01\n",
            "episode: 82/300, score: 152.3160647864549, average: -49.82, epsilon: 0.01\n",
            "episode: 83/300, score: 149.94179359676673, average: -47.44, epsilon: 0.01\n",
            "episode: 84/300, score: 161.30288097383038, average: -44.99, epsilon: 0.01\n",
            "episode: 85/300, score: 223.73134605110982, average: -41.86, epsilon: 0.01\n",
            "episode: 86/300, score: 194.0387467987503, average: -39.15, epsilon: 0.01\n",
            "episode: 87/300, score: 156.86899967775938, average: -36.92, epsilon: 0.01\n",
            "episode: 88/300, score: 35.85887120473635, average: -36.10, epsilon: 0.01\n",
            "episode: 89/300, score: 220.76009283133726, average: -33.25, epsilon: 0.01\n",
            "episode: 90/300, score: 181.09976643313792, average: -30.89, epsilon: 0.01\n",
            "episode: 91/300, score: 182.36581797387453, average: -28.58, epsilon: 0.01\n",
            "episode: 92/300, score: -76.28417673641079, average: -29.09, epsilon: 0.01\n",
            "episode: 93/300, score: 254.67802457657544, average: -26.07, epsilon: 0.01\n",
            "episode: 94/300, score: 138.9166184698136, average: -24.33, epsilon: 0.01\n",
            "episode: 95/300, score: 257.41229979033193, average: -21.40, epsilon: 0.01\n",
            "episode: 96/300, score: -105.44661462486297, average: -22.27, epsilon: 0.01\n",
            "episode: 97/300, score: -518.6292335339369, average: -27.33, epsilon: 0.01\n",
            "episode: 98/300, score: 214.32882567278662, average: -24.89, epsilon: 0.01\n",
            "episode: 99/300, score: -268.17398101112906, average: -27.32, epsilon: 0.01\n",
            "episode: 100/300, score: -42.93265655760056, average: -26.56, epsilon: 0.01\n",
            "episode: 101/300, score: 232.92674530484106, average: -22.20, epsilon: 0.01\n",
            "episode: 102/300, score: 216.1842366309417, average: -19.21, epsilon: 0.01\n",
            "episode: 103/300, score: 1.2275562602428314, average: -16.79, epsilon: 0.01\n",
            "episode: 104/300, score: 179.09238302954503, average: -13.12, epsilon: 0.01\n",
            "episode: 105/300, score: -88.9822000083839, average: -13.56, epsilon: 0.01\n",
            "episode: 106/300, score: 46.56243059431191, average: -10.07, epsilon: 0.01\n",
            "episode: 107/300, score: 236.16954265888288, average: -7.67, epsilon: 0.01\n",
            "episode: 108/300, score: 135.54266881160672, average: -5.13, epsilon: 0.01\n",
            "episode: 109/300, score: -8.654079778718284, average: -3.62, epsilon: 0.01\n",
            "episode: 110/300, score: -250.88697252033313, average: -8.07, epsilon: 0.01\n",
            "episode: 111/300, score: -181.32736948108965, average: -9.16, epsilon: 0.01\n",
            "episode: 112/300, score: -60.46362016030638, average: -9.17, epsilon: 0.01\n",
            "episode: 113/300, score: 63.428931084557334, average: -7.61, epsilon: 0.01\n",
            "episode: 114/300, score: 183.49501180740242, average: -4.92, epsilon: 0.01\n",
            "episode: 115/300, score: 68.50751850790955, average: -3.39, epsilon: 0.01\n",
            "episode: 116/300, score: 160.29232452190894, average: 1.20, epsilon: 0.01\n",
            "episode: 117/300, score: 165.60410262812678, average: 5.39, epsilon: 0.01\n",
            "episode: 118/300, score: -55.40606145804016, average: 5.12, epsilon: 0.01\n",
            "episode: 119/300, score: -67.21484902828917, average: 4.37, epsilon: 0.01\n",
            "episode: 120/300, score: 214.75606561224373, average: 6.55, epsilon: 0.01\n",
            "episode: 121/300, score: 8.177640491494932, average: 6.87, epsilon: 0.01\n",
            "episode: 122/300, score: -82.89502225485585, average: 6.52, epsilon: 0.01\n",
            "episode: 123/300, score: 45.786099799703095, average: 6.04, epsilon: 0.01\n",
            "episode: 124/300, score: -97.34966078515498, average: 7.92, epsilon: 0.01\n",
            "episode: 125/300, score: 9.427428843376177, average: 11.14, epsilon: 0.01\n",
            "episode: 126/300, score: -46.014945293497135, average: 12.29, epsilon: 0.01\n",
            "episode: 127/300, score: 189.25568981968388, average: 16.40, epsilon: 0.01\n",
            "episode: 128/300, score: 244.15280574087558, average: 21.39, epsilon: 0.01\n",
            "episode: 129/300, score: 178.1579869263473, average: 21.63, epsilon: 0.01\n",
            "episode: 130/300, score: 151.68626494125294, average: 21.82, epsilon: 0.01\n",
            "episode: 131/300, score: -61.14149394059275, average: 23.25, epsilon: 0.01\n",
            "episode: 132/300, score: 117.47364825491277, average: 26.82, epsilon: 0.01\n",
            "episode: 133/300, score: 303.86740140196343, average: 32.23, epsilon: 0.01\n",
            "episode: 134/300, score: 229.0541530067914, average: 35.52, epsilon: 0.01\n",
            "episode: 135/300, score: 86.19764954259861, average: 34.47, epsilon: 0.01\n",
            "episode: 136/300, score: 270.9644330183901, average: 39.67, epsilon: 0.01\n",
            "episode: 137/300, score: 204.83329996331372, average: 43.75, epsilon: 0.01\n",
            "episode: 138/300, score: 226.54542962498516, average: 46.42, epsilon: 0.01\n",
            "episode: 139/300, score: 167.8828236939272, average: 48.42, epsilon: 0.01\n",
            "episode: 140/300, score: 298.53805809476347, average: 50.41, epsilon: 0.01\n",
            "episode: 141/300, score: 199.77561674301006, average: 54.21, epsilon: 0.01\n",
            "episode: 142/300, score: 189.26146941253654, average: 57.06, epsilon: 0.01\n",
            "episode: 143/300, score: 187.01034012085543, average: 59.84, epsilon: 0.01\n",
            "episode: 144/300, score: -336.00600748418657, average: 54.90, epsilon: 0.01\n",
            "episode: 145/300, score: -248.91529654038652, average: 52.07, epsilon: 0.01\n",
            "episode: 146/300, score: 190.03644713148446, average: 52.65, epsilon: 0.01\n",
            "episode: 147/300, score: 220.12568174627017, average: 54.07, epsilon: 0.01\n",
            "episode: 148/300, score: -50.546995522260346, average: 53.73, epsilon: 0.01\n",
            "episode: 149/300, score: 100.60896445672518, average: 52.10, epsilon: 0.01\n",
            "episode: 150/300, score: -5.203063703903015, average: 53.63, epsilon: 0.01\n",
            "episode: 151/300, score: -325.62381060699033, average: 51.49, epsilon: 0.01\n",
            "episode: 152/300, score: -87.15011529654574, average: 52.36, epsilon: 0.01\n",
            "episode: 153/300, score: 170.23269367964122, average: 54.60, epsilon: 0.01\n",
            "episode: 154/300, score: 230.35669418025975, average: 56.65, epsilon: 0.01\n",
            "episode: 155/300, score: 201.2710387422229, average: 59.39, epsilon: 0.01\n",
            "episode: 156/300, score: 288.22320868421883, average: 62.97, epsilon: 0.01\n",
            "episode: 157/300, score: 158.36041709356405, average: 66.57, epsilon: 0.01\n",
            "episode: 158/300, score: 208.9897431255891, average: 67.43, epsilon: 0.01\n",
            "episode: 159/300, score: 265.4833587291597, average: 72.05, epsilon: 0.01\n",
            "episode: 160/300, score: 255.01323550230924, average: 73.80, epsilon: 0.01\n",
            "episode: 161/300, score: 265.0156807727769, average: 77.90, epsilon: 0.01\n",
            "episode: 162/300, score: 95.15158571646857, average: 78.97, epsilon: 0.01\n",
            "episode: 163/300, score: 259.1614324636321, average: 82.88, epsilon: 0.01\n",
            "episode: 164/300, score: 39.50007653901659, average: 83.77, epsilon: 0.01\n",
            "episode: 165/300, score: 175.31962505965024, average: 86.34, epsilon: 0.01\n",
            "episode: 166/300, score: 253.99408666830104, average: 89.81, epsilon: 0.01\n",
            "episode: 167/300, score: -129.00400109593068, average: 89.11, epsilon: 0.01\n",
            "episode: 168/300, score: -20.432259682835465, average: 87.59, epsilon: 0.01\n",
            "episode: 169/300, score: 204.8332109151015, average: 90.88, epsilon: 0.01\n",
            "episode: 170/300, score: 259.7893648082909, average: 92.69, epsilon: 0.01\n",
            "episode: 171/300, score: 116.56961463957172, average: 91.64, epsilon: 0.01\n",
            "episode: 172/300, score: 231.88933629622497, average: 94.88, epsilon: 0.01\n",
            "episode: 173/300, score: 165.81036074565577, average: 94.72, epsilon: 0.01\n",
            "episode: 174/300, score: 251.3291152729678, average: 98.82, epsilon: 0.01\n",
            "episode: 175/300, score: 5.9881802620048745, average: 98.97, epsilon: 0.01\n",
            "episode: 176/300, score: 259.2104429850721, average: 99.53, epsilon: 0.01\n",
            "episode: 177/300, score: 149.17679690266397, average: 100.74, epsilon: 0.01\n",
            "episode: 178/300, score: 116.30809976824487, average: 100.68, epsilon: 0.01\n",
            "episode: 179/300, score: 194.4774869938036, average: 100.99, epsilon: 0.01\n",
            "episode: 180/300, score: 148.52308358145757, average: 102.08, epsilon: 0.01\n",
            "episode: 181/300, score: 193.77920162397655, average: 101.95, epsilon: 0.01\n",
            "episode: 182/300, score: -196.097847621233, average: 98.47, epsilon: 0.01\n",
            "episode: 183/300, score: -40.69822740224187, average: 96.56, epsilon: 0.01\n",
            "episode: 184/300, score: -202.6696514129189, average: 92.92, epsilon: 0.01\n",
            "episode: 185/300, score: -11.852362613349683, average: 90.57, epsilon: 0.01\n",
            "episode: 186/300, score: -22.38905393331764, average: 88.40, epsilon: 0.01\n",
            "episode: 187/300, score: -205.77172598798654, average: 84.78, epsilon: 0.01\n",
            "episode: 188/300, score: -89.05950382245217, average: 83.53, epsilon: 0.01\n",
            "episode: 189/300, score: 4.09821053293382, average: 81.36, epsilon: 0.01\n",
            "episode: 190/300, score: 116.76623215900459, average: 80.72, epsilon: 0.01\n",
            "episode: 191/300, score: -90.03942340831095, average: 77.99, epsilon: 0.01\n",
            "episode: 192/300, score: 302.98106516826795, average: 81.79, epsilon: 0.01\n",
            "episode: 193/300, score: 3.3325299679072486, average: 79.27, epsilon: 0.01\n",
            "episode: 194/300, score: 267.112345892956, average: 80.55, epsilon: 0.01\n",
            "episode: 195/300, score: 176.11989539162215, average: 79.74, epsilon: 0.01\n",
            "episode: 196/300, score: 202.051330213065, average: 82.82, epsilon: 0.01\n",
            "episode: 197/300, score: 261.65336791776275, average: 90.62, epsilon: 0.01\n",
            "episode: 198/300, score: -75.38538627980186, average: 87.72, epsilon: 0.01\n",
            "episode: 199/300, score: 209.45929909720667, average: 92.50, epsilon: 0.01\n",
            "episode: 200/300, score: 246.39599571637953, average: 95.39, epsilon: 0.01\n",
            "episode: 201/300, score: -36.32405950488135, average: 92.70, epsilon: 0.01\n",
            "episode: 202/300, score: 41.74904769288316, average: 90.95, epsilon: 0.01\n",
            "episode: 203/300, score: 241.20564208560046, average: 93.35, epsilon: 0.01\n",
            "episode: 204/300, score: 131.304044208766, average: 92.88, epsilon: 0.01\n",
            "episode: 205/300, score: 266.12843841004815, average: 96.43, epsilon: 0.01\n",
            "episode: 206/300, score: 57.20712503699799, average: 96.53, epsilon: 0.01\n",
            "episode: 207/300, score: 244.22744907652816, average: 96.61, epsilon: 0.01\n",
            "episode: 208/300, score: 78.94186807073466, average: 96.05, epsilon: 0.01\n",
            "episode: 209/300, score: 277.4809519369022, average: 98.91, epsilon: 0.01\n",
            "episode: 210/300, score: 185.9766151953566, average: 103.28, epsilon: 0.01\n",
            "episode: 211/300, score: 223.88463068798347, average: 107.33, epsilon: 0.01\n",
            "episode: 212/300, score: 283.4456408235002, average: 110.77, epsilon: 0.01\n",
            "episode: 213/300, score: 1.4033253397486902, average: 110.15, epsilon: 0.01\n",
            "episode: 214/300, score: 235.546247793484, average: 110.67, epsilon: 0.01\n",
            "episode: 215/300, score: 209.55476873218137, average: 112.08, epsilon: 0.01\n",
            "episode: 216/300, score: 268.93185869020954, average: 113.17, epsilon: 0.01\n",
            "episode: 217/300, score: 255.53336296604436, average: 114.07, epsilon: 0.01\n",
            "episode: 218/300, score: -95.89760055801052, average: 113.66, epsilon: 0.01\n",
            "episode: 219/300, score: 283.32863051151, average: 117.17, epsilon: 0.01\n",
            "episode: 220/300, score: -144.0041444999016, average: 113.58, epsilon: 0.01\n",
            "episode: 221/300, score: 205.90847775587895, average: 115.56, epsilon: 0.01\n",
            "episode: 222/300, score: -39.937272867163955, average: 115.99, epsilon: 0.01\n",
            "episode: 223/300, score: -207.94412867882863, average: 113.45, epsilon: 0.01\n",
            "episode: 224/300, score: 266.0059105833182, average: 117.08, epsilon: 0.01\n",
            "episode: 225/300, score: 289.23018304548816, average: 119.88, epsilon: 0.01\n",
            "episode: 226/300, score: 254.67600444289752, average: 122.89, epsilon: 0.01\n",
            "episode: 227/300, score: 277.168761334588, average: 123.77, epsilon: 0.01\n",
            "episode: 228/300, score: 272.8297278605078, average: 124.05, epsilon: 0.01\n",
            "episode: 229/300, score: 185.19478424916116, average: 124.12, epsilon: 0.01\n",
            "episode: 230/300, score: 220.21944790491222, average: 124.81, epsilon: 0.01\n",
            "episode: 231/300, score: 239.52771305171515, average: 127.82, epsilon: 0.01\n",
            "episode: 232/300, score: 247.2557178101215, average: 129.11, epsilon: 0.01\n",
            "episode: 233/300, score: 165.2672636691703, average: 127.73, epsilon: 0.01\n",
            "episode: 234/300, score: 192.18600419969272, average: 127.36, epsilon: 0.01\n",
            "episode: 235/300, score: 265.5739045688134, average: 129.15, epsilon: 0.01\n",
            "episode: 236/300, score: 183.7993223523821, average: 128.28, epsilon: 0.01\n",
            "episode: 237/300, score: 230.27109157007857, average: 128.54, epsilon: 0.01\n",
            "episode: 238/300, score: 244.51842755423382, average: 128.71, epsilon: 0.01\n",
            "episode: 239/300, score: -15.69590952627442, average: 126.88, epsilon: 0.01\n",
            "episode: 240/300, score: 0.48800647607303915, average: 123.90, epsilon: 0.01\n",
            "episode: 241/300, score: 271.8258839777038, average: 124.62, epsilon: 0.01\n",
            "episode: 242/300, score: 189.9849446489594, average: 124.63, epsilon: 0.01\n",
            "episode: 243/300, score: 218.02287688995162, average: 124.94, epsilon: 0.01\n",
            "episode: 244/300, score: 280.72458569302853, average: 131.10, epsilon: 0.01\n",
            "episode: 245/300, score: 165.63194187310248, average: 135.25, epsilon: 0.01\n",
            "episode: 246/300, score: 153.9853577612682, average: 134.89, epsilon: 0.01\n",
            "episode: 247/300, score: 211.33946365016504, average: 134.80, epsilon: 0.01\n",
            "episode: 248/300, score: 11.577463940687508, average: 135.42, epsilon: 0.01\n",
            "episode: 249/300, score: -163.55748843073292, average: 132.78, epsilon: 0.01\n",
            "episode: 250/300, score: 216.39650521920606, average: 135.00, epsilon: 0.01\n",
            "episode: 251/300, score: 270.4524894159374, average: 140.96, epsilon: 0.01\n",
            "episode: 252/300, score: 266.55199967600583, average: 144.49, epsilon: 0.01\n",
            "episode: 253/300, score: 267.10603800702086, average: 145.46, epsilon: 0.01\n",
            "episode: 254/300, score: 256.58841943803145, average: 145.73, epsilon: 0.01\n",
            "episode: 255/300, score: 284.00801165363475, average: 146.55, epsilon: 0.01\n",
            "episode: 256/300, score: -14.683815056547402, average: 143.52, epsilon: 0.01\n",
            "episode: 257/300, score: 267.4429435741191, average: 144.61, epsilon: 0.01\n",
            "episode: 258/300, score: -145.39532258300545, average: 141.07, epsilon: 0.01\n",
            "episode: 259/300, score: 248.9701101053229, average: 140.91, epsilon: 0.01\n",
            "episode: 260/300, score: 267.26096596599837, average: 141.03, epsilon: 0.01\n",
            "episode: 261/300, score: 196.39603161725034, average: 140.34, epsilon: 0.01\n",
            "episode: 262/300, score: 262.15796785750535, average: 142.01, epsilon: 0.01\n",
            "episode: 263/300, score: 282.1514301320817, average: 142.24, epsilon: 0.01\n",
            "episode: 264/300, score: -128.58976019228257, average: 140.56, epsilon: 0.01\n",
            "episode: 265/300, score: 267.4828809640031, average: 141.48, epsilon: 0.01\n",
            "episode: 266/300, score: 212.26217471878, average: 141.07, epsilon: 0.01\n",
            "episode: 267/300, score: 228.4558289631115, average: 144.64, epsilon: 0.01\n",
            "episode: 268/300, score: 245.3411180980429, average: 147.30, epsilon: 0.01\n",
            "episode: 269/300, score: 218.4905251171757, average: 147.43, epsilon: 0.01\n",
            "episode: 270/300, score: 287.5478946497077, average: 147.71, epsilon: 0.01\n",
            "episode: 271/300, score: 196.53208268323766, average: 148.51, epsilon: 0.01\n",
            "episode: 272/300, score: 225.27555868619976, average: 148.45, epsilon: 0.01\n",
            "episode: 273/300, score: 237.1578821444537, average: 149.16, epsilon: 0.01\n",
            "episode: 274/300, score: 219.6798888704418, average: 148.84, epsilon: 0.01\n",
            "episode: 275/300, score: 186.95702178817956, average: 150.65, epsilon: 0.01\n",
            "episode: 276/300, score: 247.40331283714266, average: 150.53, epsilon: 0.01\n",
            "episode: 277/300, score: 262.30666367866445, average: 151.67, epsilon: 0.01\n",
            "episode: 278/300, score: 197.26095267368643, average: 152.47, epsilon: 0.01\n",
            "episode: 279/300, score: 256.3269052782183, average: 153.09, epsilon: 0.01\n",
            "episode: 280/300, score: 0.7252291206367687, average: 151.62, epsilon: 0.01\n",
            "episode: 281/300, score: 279.56524932129025, average: 152.47, epsilon: 0.01\n",
            "episode: 282/300, score: 230.6446527150125, average: 156.74, epsilon: 0.01\n",
            "episode: 283/300, score: 241.537438726148, average: 159.56, epsilon: 0.01\n",
            "episode: 284/300, score: 248.30597929257823, average: 164.07, epsilon: 0.01\n",
            "episode: 285/300, score: 277.85900079652896, average: 166.97, epsilon: 0.01\n",
            "episode: 286/300, score: 229.11114801718296, average: 169.48, epsilon: 0.01\n",
            "episode: 287/300, score: 261.9743127573588, average: 174.16, epsilon: 0.01\n",
            "episode: 288/300, score: 237.24169124327463, average: 177.43, epsilon: 0.01\n",
            "episode: 289/300, score: 14.753627431562904, average: 177.53, epsilon: 0.01\n",
            "episode: 290/300, score: 183.25563875390918, average: 178.20, epsilon: 0.01\n",
            "episode: 291/300, score: 251.79208173336582, average: 181.61, epsilon: 0.01\n",
            "episode: 292/300, score: 218.0669596174591, average: 180.77, epsilon: 0.01\n",
            "episode: 293/300, score: 224.27106906808785, average: 182.98, epsilon: 0.01\n",
            "episode: 294/300, score: 250.6914300023369, average: 182.81, epsilon: 0.01\n",
            "episode: 295/300, score: 254.83863882156066, average: 183.60, epsilon: 0.01\n",
            "episode: 296/300, score: 272.90316084846313, average: 184.31, epsilon: 0.01\n",
            "episode: 297/300, score: 279.0261314783867, average: 184.48, epsilon: 0.01\n",
            "episode: 298/300, score: 287.56643544819303, average: 188.11, epsilon: 0.01\n",
            "episode: 299/300, score: 279.4171362360456, average: 188.81, epsilon: 0.01\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "n_episodes = 300\n",
        "# Tuned hyperparameters\n",
        "agent = Agent(gamma=0.99, epsilon=1, lr=0.001, input_dims=[8], eps_decay=0.001,mem_size=100000,\n",
        "            batch_size=64,eps_min=0.01,fc1_dims=128,fc2_dims=128,replace=100,n_actions=4)\n",
        "\n",
        "# define score and epsilon history\n",
        "scores, eps_history = [], []\n",
        "\n",
        "for i in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        agent.store_transition(observation, action, reward, observation_, done)\n",
        "        observation = observation_\n",
        "        agent.learn()\n",
        "\n",
        "    eps_history.append(agent.epsilon)\n",
        "    scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "    print(\"episode: {}/{}, score: {}, average: {:.2f}, epsilon: {:.2f}\".format(\n",
        "        i, n_episodes, score, avg_score, agent.epsilon))\n",
        "\n",
        "file_name = 'tuned_LunarLander.png'\n",
        "agent.q_eval.save_weights('tuned_network.h5')\n",
        "\n",
        "# Show learning curves\n",
        "x = [i+1 for i in range(n_episodes)]\n",
        "plot_learning_curve(x, scores, eps_history, file_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "QIISlnLT5yMI",
        "outputId": "b20df96e-5494-4854-e0ca-ec00fc861272"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Tuned Model\n",
        "plot_scores(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lPid220azVe"
      },
      "source": [
        "Both learning curves shows a consistency in the scores/rewards increasing every episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj44txG0KP2R"
      },
      "source": [
        "<h3>Testing of Model with the tuned parameters model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuDBuSXF18BE"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn4RyFqkKW2Q"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "#! Learn from experiences/transitions, update the model\n",
        "class Agent():\n",
        "    def __init__(self,lr, gamma, n_actions, epsilon, batch_size, input_dims, eps_decay=1e-3, eps_min=0.01,\n",
        "    mem_size=100000, file_name='dueling_dqn.h5',replace=100):\n",
        "        # action = 4\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        # discount factor\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay # epsilon decay\n",
        "        self.eps_min = eps_min # min epsilon value to stop decay\n",
        "        self.file_name = file_name\n",
        "        self.batch_size = batch_size\n",
        "        self.replace = replace\n",
        "        self.learn_step_counter = 0\n",
        "        # Memory\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "        self.q_eval = DuelingDeepQNetwork(n_actions, fc1_dims=128, fc2_dims=128)\n",
        "        # Target Network with simialr architecture\n",
        "        self.q_next = DuelingDeepQNetwork(n_actions, fc1_dims=128, fc2_dims=128)\n",
        "\n",
        "        # Compile models\n",
        "        self.q_eval.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "        self.q_next.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "    # # interface function between memory and agent\n",
        "    # def store_transition(self, state, action, reward, new_state, done):\n",
        "    #     self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # # Function to choose action based on observations/environment\n",
        "    # def choose_action(self, observation):\n",
        "    #     # allow model to explore\n",
        "    #     if np.random.random() < self.epsilon:\n",
        "    #         action = np.random.choice(self.action_space)\n",
        "    #     else:\n",
        "    #         # greedy action\n",
        "    #         state = np.array([observation])\n",
        "    #         actions = self.q_eval.advantage(state)\n",
        "    #         action = tf.math.argmax(actions, axis=1).numpy()[0] #select best action\n",
        "        \n",
        "    #     return action\n",
        "    #  interface function between memory and agent\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # Function to choose action based on observations/environment\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # allow model to explore\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            # greedy action\n",
        "            state = np.array([observation])\n",
        "            actions = self.q_eval.advantage(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "    # Learning function\n",
        "    def learn(self):\n",
        "\n",
        "        #! allow model to start learning only after memory is filled up, not 0s\n",
        "        if self.memory.mem_counter < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Update target model weights\n",
        "        if self.learn_step_counter % self.replace == 0:\n",
        "            self.q_next.set_weights(self.q_eval.get_weights())\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
        "        # print(states)\n",
        "        q_pred = self.q_eval(states)\n",
        "        # print(states_)\n",
        "        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_pred)\n",
        "\n",
        "        # improve on my solution!\n",
        "        for idx, terminal in enumerate(dones):\n",
        "            if terminal:\n",
        "                q_next[idx] = 0.0\n",
        "            # for action taken, is current reward + value of next step*gamma\n",
        "            q_target[idx, actions[idx]] = rewards[idx] + self.gamma*q_next[idx]\n",
        "\n",
        "        self.q_eval.train_on_batch(states, q_target)\n",
        "\n",
        "        #decay epsilon unless it reaches min\n",
        "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
        "        #when update parameters for target network\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "    # saving the model and loading the model\n",
        "    def save_model(self):\n",
        "        tf.keras.models.save_model(self.q_eval, 'baseline_network.h5')\n",
        "        # self.q_eval.save_weights('baseline_network.h5')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.q_eval = load_model(self.model_file)\n",
        "\n",
        "\n",
        "    #Test the network function\n",
        "    def test_agent(self, env, episodes = 100):\n",
        "        self.q_eval = DuelingDeepQNetwork(4, fc1_dims=128, fc2_dims=128)\n",
        "        self.q_eval.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "        self.q_eval.build(input_shape=(1, 8))\n",
        "        self.q_eval.load_weights('tuned_network.h5')\n",
        "        total_reward = 0\n",
        "        counter = 0\n",
        "        scores = []\n",
        "        goal = 200\n",
        "        # Loop through the specified number of episodes\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            state = state[0]\n",
        "            episode_reward = 0\n",
        "            goal = 200\n",
        "            done = False\n",
        "\n",
        "            # Run one episode\n",
        "            while not done:\n",
        "                # Choose an action based on the state\n",
        "                action = self.choose_action(state)\n",
        "                \n",
        "                # Take the action in the environment and observe the next state, reward, and done flag\n",
        "                next_state, reward, done, truncated, _ = env.step(action)\n",
        "                state = next_state\n",
        "                # Update the total reward for this episode\n",
        "                episode_reward += reward\n",
        "                \n",
        "            scores.append(episode_reward)\n",
        "            \n",
        "            # Update the total reward and success rate\n",
        "            total_reward += episode_reward\n",
        "            \n",
        "        # Close the environment\n",
        "        # Compute the average reward and success rate\n",
        "        avg_reward = total_reward / episodes\n",
        "        # success_rate = success / episodes\n",
        "        \n",
        "        return avg_reward, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "COUctbAC1xyN",
        "outputId": "4acf036d-6b2e-4e06-a430-3e8aee533237"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-34f036171c81>:46: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment LunarLander-v2 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d289a6e84325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# agent.load_weights('./dueling/tuned_network.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Test the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mavg_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b40dc9d8e6ed>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Choose an action based on the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m# Take the action in the environment and observe the next state, reward, and done flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b40dc9d8e6ed>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# greedy action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-34f036171c81>\u001b[0m in \u001b[0;36madvantage\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Advantage method for choosing actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    229\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"dense_48\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (1,)"
          ]
        }
      ],
      "source": [
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "# set environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "env = gym.wrappers.RecordVideo(env, './VideoRecordLunar', episode_trigger = lambda x: x % 10 == 0)\n",
        "\n",
        "# Tuned hyperparameters, reuse the agent class with tuned parameters\n",
        "agent = Agent(gamma=0.99, epsilon=0, lr=0.001, input_dims=[8], eps_decay=0.001,mem_size=100000,\n",
        "            batch_size=64,eps_min=0.01,replace=100,n_actions=4)\n",
        "# agent.load_weights('./dueling/tuned_network.h5')\n",
        "# Test the agent\n",
        "avg_reward, scores = agent.test_agent(env)\n",
        "\n",
        "print(scores)\n",
        "print('Average reward:', avg_reward)\n",
        "#Tuned Model\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpUXWjcBazVk"
      },
      "source": [
        "<h3>References:<br><h5>https://www.youtube.com/watch?v=CoePrz751lg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d41aaf15e5d30a5ebdc9bdab9623f5c5e55247b134c1890d9af9565eff17e232"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
